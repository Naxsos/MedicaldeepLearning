{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T19:08:43.115365Z",
     "start_time": "2025-03-01T19:08:24.747939Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input, BatchNormalization, Activation, GlobalAveragePooling2D\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.applications import ResNet50, DenseNet121\n",
    "import tensorflow as tf\n",
    "import psutil\n",
    "from pympler import asizeof\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T19:08:43.309235Z",
     "start_time": "2025-03-01T19:08:43.116497Z"
    }
   },
   "outputs": [],
   "source": [
    "csv_file_path = '../DL_for_Hin_Chest_X_Ray/Data_Entry_2017_filtered_2.csv'\n",
    "csv_file_path = '../DL_for_Hin_Chest_X_Ray/HIN_archive/Data_Entry_2017.csv'\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "IMAGE_DIR = \"../DL_for_Hin_Chest_X_Ray/HIN_archive/images/\"\n",
    "ALL_LABELS = sorted(df[\"Finding Labels\"].str.split(\"|\").explode().unique())\n",
    "ALL_LABELS_WITHOUT_NO = [l for l in ALL_LABELS if l != \"No Finding\"]\n",
    "NUMBER_CLASSES = len(ALL_LABELS)\n",
    "\n",
    "def preprocess_image(file_path, image_size):\n",
    "    image = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if image is None:\n",
    "        return None\n",
    "    image = cv2.resize(image, (image_size, image_size))\n",
    "    return image\n",
    "\n",
    "def prepare_data(df, image_size, image_dir=IMAGE_DIR):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        image_path = os.path.join(image_dir, row[\"Image Index\"])\n",
    "        image = preprocess_image(image_path, image_size)\n",
    "        \n",
    "        if image is not None:\n",
    "            images.append(image)\n",
    "\n",
    "            current_label = np.zeros(NUMBER_CLASSES, dtype=int)\n",
    "        \n",
    "            if row[\"Finding Labels\"] != \"No Finding\":\n",
    "                indices = [i for i, label in enumerate(ALL_LABELS_WITHOUT_NO) if label in sorted(row[\"Finding Labels\"].split(\"|\"))]\n",
    "                for idx in indices:\n",
    "                    if 0 <= idx < NUMBER_CLASSES:\n",
    "                        current_label[idx] = 1\n",
    "            else:\n",
    "                current_label[NUMBER_CLASSES - 1] = 1\n",
    "            labels.append(current_label)\n",
    "    \n",
    "    images = np.array(images).reshape(-1, image_size, image_size)\n",
    "    images = np.repeat(images[..., np.newaxis], 3, axis=-1) #rgb\n",
    "    \n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "def prepare_data_as_paper(df, image_size, image_dir=IMAGE_DIR):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        image_path = os.path.join(image_dir, row[\"Image Index\"])\n",
    "        image = preprocess_image(image_path, image_size)\n",
    "        \n",
    "        if image is not None:\n",
    "            images.append(image)\n",
    "\n",
    "            current_label = np.zeros(NUMBER_CLASSES - 1, dtype=int)\n",
    "        \n",
    "            if row[\"Finding Labels\"] != \"No Finding\":\n",
    "                indices = [i for i, label in enumerate(ALL_LABELS_WITHOUT_NO) if label in sorted(row[\"Finding Labels\"].split(\"|\"))]\n",
    "                for idx in indices:\n",
    "                    if 0 <= idx < NUMBER_CLASSES:\n",
    "                        current_label[idx] = 1\n",
    "            labels.append(current_label)\n",
    "    \n",
    "    images = np.array(images).reshape(-1, image_size, image_size)\n",
    "    images = np.repeat(images[..., np.newaxis], 3, axis=-1) #rgb\n",
    "    \n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def weighted_cross_entropy(y_true, y_pred):\n",
    "    p = tf.reduce_sum(y_true)\n",
    "    n = tf.cast(tf.size(y_true), tf.float32) - p\n",
    "    beta_p = (p + n) / (p + 1e-7)\n",
    "    beta_n = (p + n) / (n + 1e-7)\n",
    "    loss = -beta_p * y_true * tf.math.log(y_pred + 1e-7) - beta_n * (1 - y_true) * tf.math.log(1 - y_pred + 1e-7) # Add epsilon to avoid log(0)\n",
    "    return tf.reduce_mean(loss)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-01T19:08:43.312331Z",
     "start_time": "2025-03-01T19:08:43.309934Z"
    }
   },
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def weighted_cross_entropy2(y_true, y_pred):\n",
    "    tf.print(\"y_true\", y_true, summarize=-1)\n",
    "    tf.print(\"y_pred\", y_pred, summarize=-1)\n",
    "\n",
    "    positive_cases = tf.equal(y_true[:, -1], 0)  # Positive when last column is 0\n",
    "    negative_cases = tf.equal(y_true[:, -1], 1)  # Negative when last column is 1\n",
    "    positive_cases = tf.cast(positive_cases, tf.float32)\n",
    "    negative_cases = tf.cast(negative_cases, tf.float32)\n",
    "    \n",
    "    tf.print(\"positive_cases\", positive_cases, summarize=-1)\n",
    "    tf.print(\"negative_cases\", negative_cases, summarize=-1)\n",
    "    \n",
    "    count_positive = tf.reduce_sum(positive_cases)\n",
    "    count_negative = tf.reduce_sum(negative_cases)\n",
    "    \n",
    "    tf.print(\"count_positive\", count_positive, summarize=-1)\n",
    "    tf.print(\"count_negative\", count_negative, summarize=-1)\n",
    "    \n",
    "    total_samples = tf.cast(tf.shape(y_true)[0], tf.float32)\n",
    "    \n",
    "    tf.print(\"total_samples\", total_samples)\n",
    "\n",
    "    beta_p = total_samples / count_positive\n",
    "    beta_n = total_samples / count_negative\n",
    "    \n",
    "    beta_p = tf.clip_by_value(beta_p, 1e-7, 1e7) #  Clipping \n",
    "    beta_n = tf.clip_by_value(beta_n, 1e-7, 1e7)\n",
    "    \n",
    "    tf.print(\"beta_p\", beta_p, summarize=-1)\n",
    "    tf.print(\"beta_n\", beta_n, summarize=-1)\n",
    "    \n",
    "    y_pred_binary = tf.stack([tf.reduce_sum(y_pred[:, :-1], axis=-1), y_pred[:, -1]], axis=-1)\n",
    "\n",
    "    \n",
    "    y_pred_binary = tf.cast(y_pred_binary, tf.float32)\n",
    "\n",
    "    tf.print(\"y_pred_binary\", y_pred_binary, summarize=-1)\n",
    "    \n",
    "    weighted_positive_loss = beta_p * tf.reduce_sum(-positive_cases * tf.math.log(y_pred_binary[:, 0] + 1e-7))\n",
    "    weighted_negative_loss = beta_n * tf.reduce_sum(-negative_cases * tf.math.log(y_pred_binary[:, 1] + 1e-7))\n",
    "    \n",
    "    tf.print(\"weighted_positive_loss\", weighted_positive_loss, summarize=-1)\n",
    "    tf.print(\"weighted_negative_loss\", weighted_negative_loss, summarize=-1)   \n",
    "    \n",
    "    return weighted_positive_loss + weighted_negative_loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-01T19:27:16.233438Z",
     "start_time": "2025-03-01T19:27:16.220987Z"
    }
   },
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T19:08:43.321459Z",
     "start_time": "2025-03-01T19:08:43.318628Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_resnet_model(image_size):\n",
    "    base_model = tf.keras.applications.ResNet50(include_top=False, weights='imagenet', input_shape=(image_size, image_size, 3), pooling=None)\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(base_model)\n",
    "    model.add(Conv2D(2048, (1, 1), activation=\"relu\")) # Transition Layer\n",
    "    model.add(GlobalAveragePooling2D())   \n",
    "    model.add(Dense(NUMBER_CLASSES, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss=weighted_cross_entropy, metrics=[AUC(multi_label=True)])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T19:34:09.379384Z",
     "start_time": "2025-03-01T19:32:05.712060Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/10000 [00:00<01:41, 98.69it/s]libpng warning: iCCP: profile 'ICC Profile': 'GRAY': Gray color space not permitted on RGB PNG\n",
      "100%|██████████| 10000/10000 [01:32<00:00, 107.89it/s]\n",
      "100%|██████████| 2000/2000 [00:19<00:00, 104.22it/s]\n"
     ]
    }
   ],
   "source": [
    "NUMBER_OF_IMAGES = 10000\n",
    "IMAGE_SIZE = 244\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "def normalize_image(img, label):\n",
    "    img = tf.cast(img, np.float32) / 255.0\n",
    "    return img, label\n",
    "\n",
    "images, labels = prepare_data(df[:NUMBER_OF_IMAGES], IMAGE_SIZE)\n",
    "images_train, images_val, labels_train, labels_val = sklearn.model_selection.train_test_split(images, labels, random_state=42, test_size=0.2)\n",
    "images_test, labels_test = prepare_data(df[-(int(NUMBER_OF_IMAGES / 5)):], IMAGE_SIZE)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((images_train, labels_train))\n",
    "train_dataset = train_dataset.map(normalize_image)\n",
    "train_dataset = train_dataset.shuffle(buffer_size=100)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "train_dataset = train_dataset.repeat()\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((images_val, labels_val))\n",
    "val_dataset = val_dataset.map(normalize_image)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=False)\n",
    "\n",
    "model = create_resnet_model(IMAGE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001B[1m500/500\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m79s\u001B[0m 145ms/step - auc_19: 0.6488 - loss: 0.1987 - val_auc_19: 0.6187 - val_loss: 0.2000\n",
      "Epoch 2/10\n",
      "\u001B[1m500/500\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m64s\u001B[0m 129ms/step - auc_19: 0.6583 - loss: 0.1983 - val_auc_19: 0.6250 - val_loss: 0.2115\n",
      "Epoch 3/10\n",
      "\u001B[1m500/500\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m64s\u001B[0m 128ms/step - auc_19: 0.6582 - loss: 0.1989 - val_auc_19: 0.6237 - val_loss: 0.1965\n",
      "Epoch 4/10\n",
      "\u001B[1m500/500\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m64s\u001B[0m 128ms/step - auc_19: 0.6551 - loss: 0.1982 - val_auc_19: 0.6182 - val_loss: 0.1974\n",
      "Epoch 5/10\n",
      "\u001B[1m500/500\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m64s\u001B[0m 128ms/step - auc_19: 0.6647 - loss: 0.1971 - val_auc_19: 0.6194 - val_loss: 0.1991\n",
      "Epoch 6/10\n",
      "\u001B[1m500/500\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m64s\u001B[0m 128ms/step - auc_19: 0.6645 - loss: 0.1978 - val_auc_19: 0.6281 - val_loss: 0.1981\n",
      "Epoch 7/10\n",
      "\u001B[1m500/500\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m64s\u001B[0m 127ms/step - auc_19: 0.6588 - loss: 0.1969 - val_auc_19: 0.6244 - val_loss: 0.1975\n",
      "Epoch 8/10\n",
      "\u001B[1m500/500\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m64s\u001B[0m 127ms/step - auc_19: 0.6628 - loss: 0.1971 - val_auc_19: 0.6215 - val_loss: 0.1964\n",
      "Epoch 9/10\n",
      "\u001B[1m500/500\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m64s\u001B[0m 128ms/step - auc_19: 0.6644 - loss: 0.1973 - val_auc_19: 0.6279 - val_loss: 0.2016\n",
      "Epoch 10/10\n",
      "\u001B[1m500/500\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m65s\u001B[0m 129ms/step - auc_19: 0.6670 - loss: 0.1969 - val_auc_19: 0.6238 - val_loss: 0.1969\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.src.callbacks.history.History at 0x309fb1930>"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_per_epoch = len(images_train) // BATCH_SIZE\n",
    "validation_steps = len(images_val) // BATCH_SIZE\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[AUC(multi_label=True)])\n",
    "\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=10,\n",
    "    validation_data=val_dataset,\n",
    "    validation_steps=validation_steps,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-02T12:16:14.011999Z",
     "start_time": "2025-03-02T12:05:15.958354Z"
    }
   },
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m125/125\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 92ms/step\n",
      "[0.6565615502255261, 0.5994708618914024, 0.6547018374226965, 0.4897615438292548, 0.6740580158001882, 0.5003462480911964, 0.5879024144869216, 0.8531130028375897, 0.6064896289529541, 0.5312953025254965, 0.4909565388655462, 0.4100576681511934, 0.42556073588709675, 0.569589093701997, 0.6177235733069157]\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, images_test, labels_test, batch_size=BATCH_SIZE):\n",
    "\n",
    "    images_test = tf.cast(images_test, np.float32) / 255.0\n",
    "    \n",
    "    predictions = model.predict(images_test, batch_size=batch_size)\n",
    "    \n",
    "    auc_per_class = []\n",
    "    \n",
    "    for class_idx in range(NUMBER_CLASSES):\n",
    "        true_labels_class = labels_test[:, class_idx]\n",
    "        \n",
    "        auc = sklearn.metrics.roc_auc_score(true_labels_class, predictions[:, class_idx])\n",
    "        auc_per_class.append(auc)\n",
    "    \n",
    "    return auc_per_class\n",
    "\n",
    "print(evaluate_model(model, images_test, labels_test))\n",
    "\n",
    "def add_no_finding_class(labels):\n",
    "    updated_labels = np.hstack([labels, np.zeros((labels.shape[0], 1))])  # Add a column of zeros for the 'No Finding' class\n",
    "    updated_labels[np.sum(labels, axis=1) == 0, -1] = 1\n",
    "    return updated_labels\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-02T12:32:09.701525Z",
     "start_time": "2025-03-02T12:31:56.509793Z"
    }
   },
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "All labels: 10000 samples, 50 epochs: \n",
    "[0.6692994173103355, 0.5939930395255514, 0.6683094818155433, 0.5533819389301956, 0.6756526600600586, 0.5141695372704997, 0.5874413145539906, 0.814388249040227, 0.6113292270077111, 0.5362483786464559, 0.5052575717787114, 0.39618781165543754, 0.4708606350806452, 0.5637999231950844] (No Finding missing)\n",
    "\n",
    "10 epochs:\n",
    "[0.6565615502255261, 0.5994708618914024, 0.6547018374226965, 0.4897615438292548, 0.6740580158001882, 0.5003462480911964, 0.5879024144869216, 0.8531130028375897, 0.6064896289529541, 0.5312953025254965, 0.4909565388655462, 0.4100576681511934, 0.42556073588709675, 0.569589093701997, 0.6177235733069157]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "['Atelectasis',\n 'Cardiomegaly',\n 'Consolidation',\n 'Edema',\n 'Effusion',\n 'Emphysema',\n 'Fibrosis',\n 'Hernia',\n 'Infiltration',\n 'Mass',\n 'No Finding',\n 'Nodule',\n 'Pleural_Thickening',\n 'Pneumonia',\n 'Pneumothorax']"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALL_LABELS\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-02T12:26:47.391045Z",
     "start_time": "2025-03-02T12:26:47.386710Z"
    }
   },
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medicaldeeplearning-6tfcUi4t-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
