{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T21:08:39.136260Z",
     "start_time": "2025-02-27T21:08:20.599380Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input, BatchNormalization, Activation, GlobalAveragePooling2D\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.applications import ResNet50, DenseNet121\n",
    "import tensorflow as tf\n",
    "import psutil\n",
    "from pympler import asizeof\n",
    "import sklearn\n",
    "\n",
    "# Load your CSV file\n",
    "csv_file_path = '../DL_for_Hin_Chest_X_Ray/Data_Entry_2017_filtered_2.csv'\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Initialize constants\n",
    "IMAGE_DIR = \"../DL_for_Hin_Chest_X_Ray/HIN_archive/images/\"\n",
    "\n",
    "# Initialize the multi-label binarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "unique_labels = df[\"Finding Labels\"].str.split(\"|\").explode().unique()\n",
    "mlb.fit([unique_labels])\n",
    "labels_for_class = ['Atelectasis', 'Effusion', 'Infiltration', 'No Finding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T21:08:39.146086Z",
     "start_time": "2025-02-27T21:08:39.137433Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_image(file_path, image_size):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses an image from the given file path.\n",
    "    Resizes to the specified image size and normalizes pixel values.\n",
    "    \"\"\"\n",
    "    image = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)  # Load image in grayscale\n",
    "    if image is None:\n",
    "        return None\n",
    "    image = cv2.resize(image, (image_size, image_size))\n",
    "    # image = image / 255.0  # Normalize pixel values to [0, 1]\n",
    "    return image\n",
    "\n",
    "\n",
    "def prepare_data(df, image_size, image_dir=IMAGE_DIR):\n",
    "    \"\"\"\n",
    "    Prepares images and labels from the dataset for model training.\n",
    "    - Loads images based on 'Image Index' in the dataframe.\n",
    "    - Converts 'Finding Labels' to one-hot encoded vectors.\n",
    "    - Returns arrays of images and labels.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        # Construct image path\n",
    "        image_path = os.path.join(image_dir, row[\"Image Index\"])\n",
    "        image = preprocess_image(image_path, image_size)\n",
    "        \n",
    "        if image is not None:\n",
    "            images.append(image)\n",
    "            # Convert labels into a list of diseases, then one-hot encode\n",
    "            label = row[\"Finding Labels\"].split(\"|\")\n",
    "            labels.append(label)\n",
    "    \n",
    "    # Convert lists to arrays\n",
    "    images = np.array(images).reshape(-1, image_size, image_size, 1)  # Adding channel dimension for grayscale\n",
    "    labels = mlb.transform(labels)  # Convert labels to multi-label one-hot encoding\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "def weighted_cross_entropy2(y_true, y_pred):\n",
    "    tf.print(\"y_true\", y_true, summarize=-1)\n",
    "    tf.print(\"y_pred\", y_pred, summarize=-1)\n",
    "\n",
    "    \n",
    "    positive_cases = tf.equal(y_true[:, -1], 0)  # Positive when last column is 0\n",
    "    negative_cases = tf.equal(y_true[:, -1], 1)  # Negative when last column is 1\n",
    "    positive_cases = tf.cast(positive_cases, tf.float32)\n",
    "    negative_cases = tf.cast(negative_cases, tf.float32)\n",
    "    \n",
    "    tf.print(\"positive_cases\", positive_cases, summarize=-1)\n",
    "    tf.print(\"negative_cases\", negative_cases, summarize=-1)\n",
    "    \n",
    "    count_positive = tf.reduce_sum(positive_cases)\n",
    "    count_negative = tf.reduce_sum(negative_cases)\n",
    "    \n",
    "    tf.print(\"count_positive\", count_positive, summarize=-1)\n",
    "    tf.print(\"count_negative\", count_negative, summarize=-1)\n",
    "    \n",
    "    total_samples = tf.cast(tf.shape(y_true)[0], tf.float32)\n",
    "    \n",
    "    tf.print(\"total_samples\", total_samples)\n",
    "\n",
    "    beta_p = total_samples / count_positive\n",
    "    beta_n = total_samples / count_negative\n",
    "    \n",
    "    beta_p = tf.clip_by_value(beta_p, 1e-7, 1e7) #  Clipping \n",
    "    beta_n = tf.clip_by_value(beta_n, 1e-7, 1e7)\n",
    "    \n",
    "    tf.print(\"beta_p\", beta_p, summarize=-1)\n",
    "    tf.print(\"beta_n\", beta_n, summarize=-1)\n",
    "    \n",
    "    \n",
    "    y_pred_binary = tf.stack([tf.reduce_sum(y_pred[:, :-1], axis=-1), y_pred[:, -1]], axis=-1)\n",
    "    \n",
    "    y_pred_binary = tf.cast(y_pred_binary, tf.float32)\n",
    "\n",
    "    tf.print(\"y_pred_binary\", y_pred_binary, summarize=-1)\n",
    "    \n",
    "    weighted_positive_loss = beta_p * tf.reduce_sum(-positive_cases * tf.math.log(y_pred_binary[:, 0] + 1e-7))\n",
    "    weighted_negative_loss = beta_n * tf.reduce_sum(-negative_cases * tf.math.log(y_pred_binary[:, 1] + 1e-7))\n",
    "    \n",
    "    tf.print(\"weighted_positive_loss\", weighted_positive_loss, summarize=-1)\n",
    "    tf.print(\"weighted_negative_loss\", weighted_negative_loss, summarize=-1)   \n",
    "    \n",
    "    return weighted_positive_loss + weighted_negative_loss\n",
    "\n",
    "def create_resnet_model(image_size):\n",
    "    base_model = tf.keras.applications.ResNet50(include_top=False, weights='imagenet', input_shape=(image_size, image_size, 3), pooling=None)\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(base_model)\n",
    "    model.add(Conv2D(2048, (1, 1), activation=\"relu\")) # Transition Layer\n",
    "    model.add(GlobalAveragePooling2D())   \n",
    "    model.add(Dense(4, activation='softmax', name=\"predictions\"))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[AUC()])\n",
    "    return model\n",
    "\n",
    "def evaluate(model, images_val, labels_val, num_classes):\n",
    "    # Get the model's predictions (probabilities) for the validation set\n",
    "    probabilities = model.predict(images_val)\n",
    "\n",
    "    # Initialize an empty list to store the AUCs\n",
    "    auc_per_class = {}\n",
    "\n",
    "    # Calculate AUC for each class\n",
    "    for class_idx in range(num_classes):\n",
    "        true_labels = labels_val[:, class_idx]  # True labels for this class\n",
    "        pred_probs = probabilities[:, class_idx]  # Predicted probabilities for this class\n",
    "\n",
    "        # Calculate the AUC for this class\n",
    "        auc = sklearn.metrics.roc_auc_score(true_labels, pred_probs)\n",
    "        auc_per_class[labels_for_class[class_idx]] = auc        \n",
    "        \n",
    "    probabilities_transformed = probabilities.argmax(axis=1)\n",
    "    labels_val_transformed = labels_val.argmax(axis=1)\n",
    "\n",
    "    balanced_acc = sklearn.metrics.balanced_accuracy_score(labels_val_transformed, probabilities_transformed)\n",
    "    acc = sklearn.metrics.accuracy_score(labels_val_transformed, probabilities_transformed)\n",
    "\n",
    "    return auc_per_class, balanced_acc, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 8746/40000 [01:29<05:22, 96.94it/s] libpng warning: iCCP: profile 'ICC Profile': 'GRAY': Gray color space not permitted on RGB PNG\n",
      "100%|██████████| 40000/40000 [06:49<00:00, 97.57it/s] \n",
      "100%|██████████| 8000/8000 [01:30<00:00, 88.53it/s] \n"
     ]
    }
   ],
   "source": [
    "IMAGE_SIZE = 224\n",
    "NUMBER_OF_IMAGES = 40000\n",
    "\n",
    "def normalize_image(img, label):\n",
    "    img = tf.cast(img, np.float32) / 255.0\n",
    "    return img, label\n",
    "\n",
    "images, labels = prepare_data(df[:NUMBER_OF_IMAGES], IMAGE_SIZE)\n",
    "images_test, labels_test = prepare_data(df[-(int(NUMBER_OF_IMAGES / 5)):], IMAGE_SIZE)\n",
    "\n",
    "\n",
    "################# only used if model requires 3 channels\n",
    "images = np.repeat(images[..., np.newaxis], 3, axis=-1).reshape(-1, IMAGE_SIZE, IMAGE_SIZE, 3)\n",
    "images_test = np.repeat(images_test[..., np.newaxis], 3, axis=-1).reshape(-1, IMAGE_SIZE, IMAGE_SIZE, 3)\n",
    "\n",
    "images_train, images_val, labels_train, labels_val = sklearn.model_selection.train_test_split(images, labels, random_state=42, test_size=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-27T21:17:20.077343Z",
     "start_time": "2025-02-27T21:08:39.147100Z"
    }
   },
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 22:17:46.298654: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n",
      "2025-02-27 22:17:46.299650: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2025-02-27 22:17:46.300968: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2025-02-27 22:17:46.301938: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-02-27 22:17:46.302549: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 22:18:23.607861: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 - 254s - 254ms/step - auc_1: 0.8341 - loss: 0.9116 - val_auc_1: 0.8445 - val_loss: 0.8857\n",
      "Epoch 2/20\n",
      "1000/1000 - 238s - 238ms/step - auc_1: 0.8406 - loss: 0.8843 - val_auc_1: 0.8483 - val_loss: 0.8823\n",
      "Epoch 3/20\n",
      "1000/1000 - 245s - 245ms/step - auc_1: 0.8453 - loss: 0.8784 - val_auc_1: 0.8528 - val_loss: 0.8798\n",
      "Epoch 4/20\n",
      "1000/1000 - 241s - 241ms/step - auc_1: 0.8500 - loss: 0.8728 - val_auc_1: 0.8572 - val_loss: 0.8723\n",
      "Epoch 5/20\n",
      "1000/1000 - 238s - 238ms/step - auc_1: 0.8538 - loss: 0.8674 - val_auc_1: 0.8596 - val_loss: 0.8646\n",
      "Epoch 6/20\n",
      "1000/1000 - 239s - 239ms/step - auc_1: 0.8569 - loss: 0.8629 - val_auc_1: 0.8622 - val_loss: 0.8609\n",
      "Epoch 7/20\n",
      "1000/1000 - 238s - 238ms/step - auc_1: 0.8593 - loss: 0.8585 - val_auc_1: 0.8639 - val_loss: 0.8562\n",
      "Epoch 8/20\n",
      "1000/1000 - 239s - 239ms/step - auc_1: 0.8607 - loss: 0.8563 - val_auc_1: 0.8651 - val_loss: 0.8534\n",
      "Epoch 9/20\n",
      "1000/1000 - 238s - 238ms/step - auc_1: 0.8623 - loss: 0.8538 - val_auc_1: 0.8667 - val_loss: 0.8498\n",
      "Epoch 10/20\n",
      "1000/1000 - 240s - 240ms/step - auc_1: 0.8630 - loss: 0.8521 - val_auc_1: 0.8670 - val_loss: 0.8500\n",
      "Epoch 11/20\n",
      "1000/1000 - 350s - 350ms/step - auc_1: 0.8647 - loss: 0.8493 - val_auc_1: 0.8675 - val_loss: 0.8513\n",
      "Epoch 12/20\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "    \n",
    "dataset = tf.data.Dataset.from_tensor_slices((images_train, labels_train))\n",
    "dataset = dataset.map(normalize_image)\n",
    "dataset = dataset.shuffle(buffer_size=100)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset = dataset.repeat()\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((images_val, labels_val))\n",
    "val_dataset = val_dataset.map(normalize_image)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=False)\n",
    "    \n",
    "model = create_resnet_model(IMAGE_SIZE)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[AUC()])\n",
    "\n",
    "class_weights = sklearn.utils.class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(labels_train.argmax(axis=1)), y=labels_train.argmax(axis=1))\n",
    "class_weights = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "history = model.fit(dataset, epochs=20, batch_size=BATCH_SIZE, steps_per_epoch=len(images_train) // BATCH_SIZE, validation_data=val_dataset, validation_steps=len(images_val)//BATCH_SIZE, callbacks=[early_stopping], verbose=2)#, class_weight=class_weights)\n",
    "\n",
    "evals = evaluate(model, images_test, labels_test, len(unique_labels))\n",
    "print(evals)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-02-27T21:17:20.089215Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "font_size = 16\n",
    "\n",
    "plt.plot(history.history['loss'], label=\"Loss\")\n",
    "plt.plot(history.history['val_loss'], label=\"Validation Loss\")\n",
    "\n",
    "plt.xlabel(\"Epochs\", fontsize=font_size)\n",
    "plt.grid(color='grey', linestyle='-', linewidth=0.25, alpha=0.5)\n",
    "plt.legend(fontsize=font_size, loc=\"center right\")\n",
    "\n",
    "plt.savefig(\"evals/history_256px_1000samples_model3.pdf\", bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Resolution\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "NUMBER_OF_IMAGES = 1000\n",
    "evaluations = {}\n",
    "\n",
    "def normalize_image(img, label):\n",
    "    img = tf.cast(img, np.float32) / 255.0\n",
    "    return img, label\n",
    "\n",
    "\n",
    "for ev_image_size in [50, 100, 150, 200, 250, 300, 350, 400]:\n",
    "    \n",
    "    images, labels = prepare_data(df[:NUMBER_OF_IMAGES], ev_image_size)\n",
    "    images_test, labels_test = prepare_data(df[-(int(NUMBER_OF_IMAGES / 5)):], ev_image_size)\n",
    "\n",
    "    images_train, images_val, labels_train, labels_val = sklearn.model_selection.train_test_split(images, labels, random_state=42, test_size=0.2)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images_train, labels_train))\n",
    "    dataset = dataset.map(normalize_image)\n",
    "    dataset = dataset.shuffle(buffer_size=100)\n",
    "    dataset = dataset.batch(BATCH_SIZE, drop_remainder=False)\n",
    "    dataset = dataset.repeat()\n",
    "    \n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((images_val, labels_val))\n",
    "    val_dataset = val_dataset.map(normalize_image)\n",
    "    val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=False)\n",
    "        \n",
    "    model = create_model_3(ev_image_size)\n",
    "        \n",
    "    class_weights = sklearn.utils.class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(labels_train.argmax(axis=1)), y=labels_train.argmax(axis=1))\n",
    "    class_weights = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    model.fit(dataset, epochs=20, batch_size=BATCH_SIZE, steps_per_epoch=len(images_train) // BATCH_SIZE, validation_data=val_dataset, validation_steps=len(images_val)//BATCH_SIZE, callbacks=[early_stopping], class_weight=class_weights, verbose=2)\n",
    "    \n",
    "    evals = evaluate(model, images_test, labels_test, len(unique_labels))\n",
    "    \n",
    "    evaluations[ev_image_size] = evals\n",
    "    print()\n",
    "    print(evals)\n",
    "    print()\n",
    "    \n",
    "print(evaluations)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Amount of Images\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUMBER_OF_IMAGES = 40000\n",
    "IMAGE_SIZE = 224\n",
    "\n",
    "images_global, labels_global = prepare_data(df[:NUMBER_OF_IMAGES], IMAGE_SIZE)\n",
    "images_global = np.repeat(images_global[..., np.newaxis], 3, axis=-1).reshape(-1, IMAGE_SIZE, IMAGE_SIZE, 3)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "evaluations = {}\n",
    "\n",
    "def normalize_image(img, label):\n",
    "    img = tf.cast(img, np.float32) / 255.0\n",
    "    return img, label\n",
    "\n",
    "\n",
    "for ev_number_of_images in [1000, 10000, 20000, 30000, 40000]:\n",
    "        \n",
    "    images, labels = images_global[:ev_number_of_images], labels_global[:ev_number_of_images]\n",
    "    images_test, labels_test = images_global[-(int(ev_number_of_images / 5)):], labels_global[-(int(ev_number_of_images / 5)):]\n",
    "\n",
    "    images_train, images_val, labels_train, labels_val = sklearn.model_selection.train_test_split(images, labels, random_state=42, test_size=0.2)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images_train, labels_train))\n",
    "    dataset = dataset.map(normalize_image)\n",
    "    dataset = dataset.shuffle(buffer_size=100)\n",
    "    dataset = dataset.batch(BATCH_SIZE, drop_remainder=False)\n",
    "    dataset = dataset.repeat()\n",
    "    \n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((images_val, labels_val))\n",
    "    val_dataset = val_dataset.map(normalize_image)\n",
    "    val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=False)\n",
    "        \n",
    "    model = create_resnet_model(IMAGE_SIZE)\n",
    "        \n",
    "    class_weights = sklearn.utils.class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(labels_train.argmax(axis=1)), y=labels_train.argmax(axis=1))\n",
    "    class_weights = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    model.fit(dataset, epochs=20, batch_size=BATCH_SIZE, steps_per_epoch=len(images_train) // BATCH_SIZE, validation_data=val_dataset, validation_steps=len(images_val)//BATCH_SIZE, callbacks=[early_stopping], verbose=2, class_weight=class_weights)\n",
    "    \n",
    "    evals = evaluate(model, images_test, labels_test, len(unique_labels))\n",
    "    \n",
    "    evaluations[ev_number_of_images] = evals\n",
    "    print()\n",
    "    print(evals)\n",
    "    print()\n",
    "    \n",
    "print(evaluations)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "with open(\"pickles/resolution_1000samples_nolr_model3.pkl\", \"wb\") as file:\n",
    "    pickle.dump(evaluations, file)\n",
    "\n",
    "aucs = [evalu[0] for evalu in evaluations.values()]\n",
    "aucs_at = [evalu[\"Atelectasis\"] for evalu in aucs]\n",
    "aucs_ef = [evalu[\"Effusion\"] for evalu in aucs]\n",
    "aucs_in = [evalu[\"Infiltration\"] for evalu in aucs]\n",
    "aucs_nf = [evalu[\"No Finding\"] for evalu in aucs]\n",
    "baccs = [evalu[1] for evalu in evaluations.values()]\n",
    "accs = [evalu[2] for evalu in evaluations.values()]\n",
    "\n",
    "x_values = [50, 100, 150, 200, 250, 300, 350, 400]\n",
    "\n",
    "font_size = 16\n",
    "\n",
    "# plt.plot(x_values, accs, label=\"Accuracy\")\n",
    "# plt.plot(x_values, baccs, label=\"Balanced Accuracy\")\n",
    "\n",
    "plt.plot(x_values, aucs_at, label=\"Atelectasis\")\n",
    "plt.plot(x_values, aucs_ef, label=\"Effusion\")\n",
    "plt.plot(x_values, aucs_in, label=\"Infiltration\")\n",
    "plt.plot(x_values, aucs_nf, label=\"No Finding\")\n",
    "\n",
    "plt.xlabel(\"Resolution in pixels x pixels\", fontsize=font_size)\n",
    "plt.grid(color='grey', linestyle='-', linewidth=0.25, alpha=0.5)\n",
    "plt.legend(fontsize=font_size, loc=\"lower right\")\n",
    "\n",
    "#plt.savefig(\"evals/amount_resnet.pdf\", bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Baseline\n",
    "\n",
    "images, labels = prepare_data(df[:10000], 1)\n",
    "\n",
    "# Calculate class probabilities\n",
    "class_counts = np.sum(labels, axis=0)\n",
    "class_probabilities = class_counts / len(labels)\n",
    "\n",
    "# Prepare the test data\n",
    "images_test, labels_test = prepare_data(df[-2000:], 1)\n",
    "\n",
    "# Generate predictions based on class probabilities\n",
    "predictions = []\n",
    "for _ in range(len(labels_test)):\n",
    "    predicted_class = np.random.choice(len(class_probabilities), p=class_probabilities)\n",
    "    predictions.append(predicted_class)\n",
    "    \n",
    "predictions_proba = np.zeros((len(labels_test), len(class_probabilities)))\n",
    "for i, predicted_class in enumerate(predictions):\n",
    "    predictions_proba[i, predicted_class] = 1\n",
    "    \n",
    "evals = []\n",
    "for class_idx in range(4):\n",
    "    true_labels = labels_test[:, class_idx]  # True labels for this class\n",
    "    pred_probs = predictions_proba[:, class_idx]  # Predicted probabilities for this class\n",
    "\n",
    "    # Calculate the AUC for this class\n",
    "    auc = sklearn.metrics.roc_auc_score(true_labels, pred_probs)\n",
    "    evals.append(auc)\n",
    "    print(f\"AUC for class {class_idx}: {auc}\")\n",
    "    \n",
    "    \n",
    "probabilities_transformed = predictions\n",
    "labels_val_transformed = labels_test.argmax(axis=1)\n",
    "\n",
    "balanced_acc = sklearn.metrics.balanced_accuracy_score(labels_val_transformed, probabilities_transformed)\n",
    "acc = sklearn.metrics.accuracy_score(labels_val_transformed, probabilities_transformed)\n",
    "\n",
    "print(balanced_acc)\n",
    "print(acc)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# tf.keras.utils.plot_model(create_model_2(256), show_shapes=False, rankdir=\"LR\")\n",
    "\n",
    "birne = create_model_3(256)\n",
    "def myprint(s):\n",
    "    with open('test.txt','w') as f:\n",
    "        print(s, file=f)\n",
    "\n",
    "birne.summary(print_fn=myprint)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Train for balanced accuracy\n",
    "class_weights = {}\n",
    "num_classes = len(unique_labels)\n",
    "\n",
    "for class_idx in range(num_classes):\n",
    "    class_count = np.sum(labels[:, class_idx])\n",
    "    class_weight = len(labels) / (num_classes * (class_count + 1e-5)) \n",
    "    class_weights[class_idx] = class_weight\n",
    "\n",
    "\n",
    "model.fit(dataset, epochs=10, batch_size=BATCH_SIZE, steps_per_epoch=len(images) // BATCH_SIZE, class_weight=class_weights, validation_data=(images_val, labels_val))\n",
    "probabilities = model.predict(images_test)\n",
    "predictions = probabilities.argmax(axis=1)\n",
    "labels_test = labels_test.argmax(axis=1)\n",
    "\n",
    "balanced_acc = sklearn.metrics.balanced_accuracy_score(labels_test, predictions)\n",
    "\n",
    "balanced_acc"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medicaldeeplearning-6tfcUi4t-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
