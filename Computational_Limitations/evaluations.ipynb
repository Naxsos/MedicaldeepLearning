{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T16:38:38.888246Z",
     "start_time": "2025-02-13T16:38:38.883337Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Cardiomegaly', 'Emphysema', 'Effusion', 'No Finding', 'Hernia',\n",
       "       'Infiltration', 'Mass', 'Nodule', 'Atelectasis', 'Pneumothorax',\n",
       "       'Pleural_Thickening', 'Pneumonia', 'Fibrosis', 'Edema',\n",
       "       'Consolidation'], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load your CSV file\n",
    "csv_file_path = '../DL_for_Hin_Chest_X_Ray/HIN_archive/Data_Entry_2017.csv'\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Initialize constants\n",
    "IMAGE_DIR = \"../DL_for_Hin_Chest_X_Ray/HIN_archive/images/\"\n",
    "IMAGE_SIZE = (256, 256)\n",
    "\n",
    "# Initialize the multi-label binarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "unique_labels = df[\"Finding Labels\"].str.split(\"|\").explode().unique()\n",
    "mlb.fit([unique_labels])\n",
    "unique_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(file_path, image_size=IMAGE_SIZE):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses an image from the given file path.\n",
    "    Resizes to the specified image size and normalizes pixel values.\n",
    "    \"\"\"\n",
    "    image = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)  # Load image in grayscale\n",
    "    if image is None:\n",
    "        return None\n",
    "    image = cv2.resize(image, image_size)\n",
    "    image = image / 255.0  # Normalize pixel values to [0, 1]\n",
    "    return image\n",
    "\n",
    "\n",
    "def prepare_data(df, image_dir=IMAGE_DIR, image_size=IMAGE_SIZE):\n",
    "    \"\"\"\n",
    "    Prepares images and labels from the dataset for model training.\n",
    "    - Loads images based on 'Image Index' in the dataframe.\n",
    "    - Converts 'Finding Labels' to one-hot encoded vectors.\n",
    "    - Returns arrays of images and labels.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        # Construct image path\n",
    "        image_path = os.path.join(image_dir, row[\"Image Index\"])\n",
    "        image = preprocess_image(image_path, image_size)\n",
    "        \n",
    "        if image is not None:\n",
    "            images.append(image)\n",
    "            # Convert labels into a list of diseases, then one-hot encode\n",
    "            label = row[\"Finding Labels\"].split(\"|\")\n",
    "            labels.append(label)\n",
    "    \n",
    "    # Convert lists to arrays\n",
    "    images = np.array(images).reshape(-1, image_size[0], image_size[1], 1)  # Adding channel dimension for grayscale\n",
    "    labels = mlb.transform(labels)  # Convert labels to multi-label one-hot encoding\n",
    "    \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(image_size_x, image_size_y):\n",
    "\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(image_size_x, image_size_y, 1)),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(len(unique_labels), activation='sigmoid')  # For multi-label classification\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: profile 'ICC Profile': 'GRAY': Gray color space not permitted on RGB PNG\n",
      "libpng warning: iCCP: profile 'ICC Profile': 'GRAY': Gray color space not permitted on RGB PNG\n"
     ]
    }
   ],
   "source": [
    "images, labels = prepare_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/I569354/Library/Caches/pypoetry/virtualenvs/medicaldeeplearning-6tfcUi4t-py3.10/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-14 16:39:27.149687: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 369ms/step - accuracy: 0.1477 - loss: 1.1117 - val_accuracy: 0.1333 - val_loss: 0.3369\n",
      "Epoch 2/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 200ms/step - accuracy: 0.0953 - loss: 0.4827 - val_accuracy: 0.1333 - val_loss: 0.2911\n",
      "Epoch 3/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 177ms/step - accuracy: 0.1122 - loss: 0.4206 - val_accuracy: 0.3667 - val_loss: 0.2900\n",
      "Epoch 4/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 175ms/step - accuracy: 0.1978 - loss: 0.3707 - val_accuracy: 0.3667 - val_loss: 0.3073\n",
      "Epoch 5/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 186ms/step - accuracy: 0.2571 - loss: 0.3315 - val_accuracy: 0.2500 - val_loss: 0.3076\n",
      "Epoch 6/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 238ms/step - accuracy: 0.3105 - loss: 0.3113 - val_accuracy: 0.3500 - val_loss: 0.3053\n",
      "Epoch 7/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 177ms/step - accuracy: 0.3710 - loss: 0.2880 - val_accuracy: 0.3500 - val_loss: 0.3215\n",
      "Epoch 8/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 181ms/step - accuracy: 0.4016 - loss: 0.2727 - val_accuracy: 0.2833 - val_loss: 0.3485\n",
      "Epoch 9/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 186ms/step - accuracy: 0.5070 - loss: 0.2409 - val_accuracy: 0.3667 - val_loss: 0.3684\n",
      "Epoch 10/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 174ms/step - accuracy: 0.4529 - loss: 0.2030 - val_accuracy: 0.3167 - val_loss: 0.4132\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 130ms/step - accuracy: 0.7202 - loss: 0.1378\n",
      "Epoch 1/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 147ms/step - accuracy: 0.1621 - loss: 0.6119 - val_accuracy: 0.3667 - val_loss: 0.3485\n",
      "Epoch 2/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.2012 - loss: 0.3724 - val_accuracy: 0.3667 - val_loss: 0.2919\n",
      "Epoch 3/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.2264 - loss: 0.3615 - val_accuracy: 0.3667 - val_loss: 0.2927\n",
      "Epoch 4/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.2048 - loss: 0.3517 - val_accuracy: 0.3667 - val_loss: 0.2929\n",
      "Epoch 5/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.1537 - loss: 0.3405 - val_accuracy: 0.3667 - val_loss: 0.2867\n",
      "Epoch 6/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.2320 - loss: 0.3173 - val_accuracy: 0.3667 - val_loss: 0.2879\n",
      "Epoch 7/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.2547 - loss: 0.3169 - val_accuracy: 0.3833 - val_loss: 0.2954\n",
      "Epoch 8/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.2982 - loss: 0.3108 - val_accuracy: 0.4000 - val_loss: 0.3013\n",
      "Epoch 9/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.2685 - loss: 0.3159 - val_accuracy: 0.3000 - val_loss: 0.3011\n",
      "Epoch 10/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.2682 - loss: 0.3148 - val_accuracy: 0.3500 - val_loss: 0.2893\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.3593 - loss: 0.2799\n",
      "Epoch 1/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 194ms/step - accuracy: 0.1132 - loss: 0.5237 - val_accuracy: 0.3667 - val_loss: 0.3123\n",
      "Epoch 2/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.1524 - loss: 0.3917 - val_accuracy: 0.3667 - val_loss: 0.2900\n",
      "Epoch 3/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.2073 - loss: 0.3691 - val_accuracy: 0.3667 - val_loss: 0.3056\n",
      "Epoch 4/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.2297 - loss: 0.3376 - val_accuracy: 0.3667 - val_loss: 0.2914\n",
      "Epoch 5/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.2194 - loss: 0.3262 - val_accuracy: 0.3667 - val_loss: 0.2975\n",
      "Epoch 6/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.2369 - loss: 0.3220 - val_accuracy: 0.3667 - val_loss: 0.2958\n",
      "Epoch 7/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.2688 - loss: 0.3097 - val_accuracy: 0.3667 - val_loss: 0.3147\n",
      "Epoch 8/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.2362 - loss: 0.3102 - val_accuracy: 0.3500 - val_loss: 0.3027\n",
      "Epoch 9/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.2389 - loss: 0.3062 - val_accuracy: 0.3500 - val_loss: 0.3009\n",
      "Epoch 10/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3232 - loss: 0.3002 - val_accuracy: 0.3500 - val_loss: 0.2967\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.3478 - loss: 0.2790\n",
      "{((256, 256), 0.05): [0.1861838549375534, 0.6366666555404663], ((128, 128), 0.05): [0.277112752199173, 0.3733333349227905], ((64, 64), 0.05): [0.2761671841144562, 0.36666667461395264]}\n"
     ]
    }
   ],
   "source": [
    "image_sizes = [(256, 256), (128, 128), (64, 64)]\n",
    "percentages = [0.05]\n",
    "results = {}\n",
    "\n",
    "for image_size in image_sizes:\n",
    "    for pct in percentages:\n",
    "        # Resize images\n",
    "        resized_images = np.array(images).reshape(-1, image_size[0], image_size[1], 1)\n",
    "        # Slice data based on percentage\n",
    "        subset_images = resized_images[:int(pct * len(images))]\n",
    "        subset_labels = labels[:int(pct * len(labels))]\n",
    "\n",
    "        model = create_model(image_size[0], image_size[1])\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(subset_images, subset_labels, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "        # Evaluate and store accuracy\n",
    "        accuracy = model.evaluate(subset_images, subset_labels)\n",
    "        results[(image_size, pct)] = accuracy\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medicaldeeplearning-6tfcUi4t-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
