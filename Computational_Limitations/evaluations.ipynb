{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T20:45:53.336151Z",
     "start_time": "2025-02-27T20:45:48.061606Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input, BatchNormalization, Activation, GlobalAveragePooling2D\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.applications import ResNet50, DenseNet121\n",
    "import tensorflow as tf\n",
    "import psutil\n",
    "from pympler import asizeof\n",
    "import sklearn\n",
    "\n",
    "# Load your CSV file\n",
    "csv_file_path = '../DL_for_Hin_Chest_X_Ray/Data_Entry_2017_filtered_2.csv'\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Initialize constants\n",
    "IMAGE_DIR = \"../DL_for_Hin_Chest_X_Ray/HIN_archive/images/\"\n",
    "\n",
    "# Initialize the multi-label binarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "unique_labels = df[\"Finding Labels\"].str.split(\"|\").explode().unique()\n",
    "mlb.fit([unique_labels])\n",
    "labels_for_class = ['Atelectasis', 'Effusion', 'Infiltration', 'No Finding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T20:45:53.352846Z",
     "start_time": "2025-02-27T20:45:53.337720Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_image(file_path, image_size):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses an image from the given file path.\n",
    "    Resizes to the specified image size and normalizes pixel values.\n",
    "    \"\"\"\n",
    "    image = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)  # Load image in grayscale\n",
    "    if image is None:\n",
    "        return None\n",
    "    image = cv2.resize(image, (image_size, image_size))\n",
    "    # image = image / 255.0  # Normalize pixel values to [0, 1]\n",
    "    return image\n",
    "\n",
    "\n",
    "def prepare_data(df, image_size, image_dir=IMAGE_DIR):\n",
    "    \"\"\"\n",
    "    Prepares images and labels from the dataset for model training.\n",
    "    - Loads images based on 'Image Index' in the dataframe.\n",
    "    - Converts 'Finding Labels' to one-hot encoded vectors.\n",
    "    - Returns arrays of images and labels.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        # Construct image path\n",
    "        image_path = os.path.join(image_dir, row[\"Image Index\"])\n",
    "        image = preprocess_image(image_path, image_size)\n",
    "        \n",
    "        if image is not None:\n",
    "            images.append(image)\n",
    "            # Convert labels into a list of diseases, then one-hot encode\n",
    "            label = row[\"Finding Labels\"].split(\"|\")\n",
    "            labels.append(label)\n",
    "    \n",
    "    # Convert lists to arrays\n",
    "    images = np.array(images).reshape(-1, image_size, image_size, 1)  # Adding channel dimension for grayscale\n",
    "    labels = mlb.transform(labels)  # Convert labels to multi-label one-hot encoding\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "def weighted_cross_entropy_loss(y_true, y_pred):\n",
    "    \n",
    "    tf.print(\"y_true\", y_true, summarize=-1)\n",
    "    tf.print(\"y_pred\", y_pred, summarize=-1)\n",
    "\n",
    "    positive_cases = tf.equal(y_true[:, -1], 0)  # Positive when last column is 0\n",
    "    negative_cases = tf.equal(y_true[:, -1], 1)  # Negative when last column is 1\n",
    "    \n",
    "    tf.print(\"positve_cases\", positive_cases, summarize=-1)\n",
    "    tf.print(\"negative_cases\", negative_cases, summarize=-1)\n",
    "    \n",
    "    count_positive = tf.reduce_sum(tf.cast(positive_cases, tf.float32))\n",
    "    count_negative = tf.reduce_sum(tf.cast(negative_cases, tf.float32))\n",
    "    \n",
    "    tf.print(\"count_positive\", count_positive, summarize=-1)\n",
    "    tf.print(\"count_negative\", count_negative, summarize=-1)\n",
    "    \n",
    "    total_samples = tf.cast(tf.shape(y_true)[0], tf.float32)\n",
    "    \n",
    "    tf.print(\"total_samples\", total_samples)\n",
    "    \n",
    "    # Calculate weights for positive and negative cases\n",
    "    beta_p = (total_samples) / count_positive\n",
    "    beta_n = (total_samples) / count_negative\n",
    "    \n",
    "    # Clip to prevent division by zero if there are no positive/negative samples (edge case)\n",
    "    beta_p = tf.clip_by_value(beta_p, 1e-7, 1e7) # Small values to avoid NaNs\n",
    "    beta_n = tf.clip_by_value(beta_n, 1e-7, 1e7)\n",
    "    \n",
    "    tf.print(\"beta_p\", beta_p, summarize=-1)\n",
    "    tf.print(\"beta_n\", beta_n, summarize=-1)\n",
    "    \n",
    "    y_true_binary = tf.stack([positive_cases, negative_cases], axis=-1)\n",
    "    y_pred_binary = tf.stack([tf.reduce_sum(y_pred[:, :-1], axis=-1), y_pred[:, -1]], axis=-1)\n",
    "    \n",
    "    y_true_binary = tf.cast(y_true_binary, tf.float32)\n",
    "    y_pred_binary = tf.cast(y_pred_binary, tf.float32)\n",
    "    positive_cases = tf.cast(positive_cases, tf.float32)\n",
    "    negative_cases = tf.cast(negative_cases, tf.float32)\n",
    "    \n",
    "    tf.print(\"y_true_binary\", y_true_binary, summarize=-1)\n",
    "    tf.print(\"y_pred_binary\", y_pred_binary, summarize=-1)\n",
    "    \n",
    "    # Apply weights for positive and negative cases\n",
    "    weighted_positive_loss = beta_p * tf.reduce_sum(-positive_cases * tf.math.log(y_pred_binary[:, 0] + 1e-7))\n",
    "    weighted_negative_loss = beta_n * tf.reduce_sum(-negative_cases * tf.math.log(y_pred_binary[:, 1] + 1e-7))\n",
    "    \n",
    "    tf.print(\"weighted_positive_loss\", weighted_positive_loss, summarize=-1)\n",
    "    tf.print(\"weighted_negative_loss\", weighted_negative_loss, summarize=-1)\n",
    "\n",
    "    # Total weighted cross-entropy loss\n",
    "    total_loss = weighted_positive_loss + weighted_negative_loss\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def create_model_1(image_size):\n",
    "    model = Sequential([\n",
    "        Input(shape=(image_size, image_size, 1)),\n",
    "        Conv2D(32, (3, 3), activation='relu'),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.25),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(len(unique_labels), activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-6), loss='categorical_crossentropy', metrics=[AUC()])\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_model_2(image_size):\n",
    "    model = Sequential([\n",
    "        Input(shape=(image_size, image_size, 1)),\n",
    "        Conv2D(32, (3, 3), activation='relu'),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        # BatchNormalization(), # new\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.25),\n",
    "        Conv2D(128, (3, 3), activation='relu'), # new\n",
    "        Conv2D(128, (3, 3), activation='relu'), # new\n",
    "        # BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)), # new\n",
    "        Dropout(0.3), # new\n",
    "        Flatten(),\n",
    "        Dense(256, activation='relu'), # former 128\n",
    "        Dropout(0.5),\n",
    "        Dense(len(unique_labels), activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=[AUC()])\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_model_3(image_size):\n",
    "    model = Sequential([\n",
    "        Input(shape=(image_size, image_size, 1)),\n",
    "        Conv2D(32, (3, 3), activation='relu'),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.25),\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.3),\n",
    "        Conv2D(256, (3, 3), activation='relu', padding='same'), # new\n",
    "        Conv2D(256, (3, 3), activation='relu', padding='same'), # new\n",
    "        MaxPooling2D((2, 2)), # new\n",
    "        Dropout(0.3), # new\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'), # former 256\n",
    "        Dropout(0.5),\n",
    "        Dense(len(unique_labels), activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=[AUC()])\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_model_4(image_size):\n",
    "    model = Sequential([\n",
    "        Input(shape=(image_size, image_size, 1)),\n",
    "        Conv2D(64, (3, 3), activation='relu'), # former 32\n",
    "        Conv2D(128, (3, 3), activation='relu'), # former 64\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.2),\n",
    "        Conv2D(256, (3, 3), activation='relu'), # former 128\n",
    "        Conv2D(256, (3, 3), activation='relu'), # former 128\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.25),\n",
    "        Conv2D(512, (3, 3), activation='relu', padding='same'), # former 256\n",
    "        Conv2D(512, (3, 3), activation='relu', padding='same'), # former 256 \n",
    "        MaxPooling2D((2, 2)), \n",
    "        Dropout(0.3), \n",
    "        GlobalAveragePooling2D(), # former Flatten\n",
    "        Dense(1024, activation='relu'), # former 512\n",
    "        Dropout(0.4),\n",
    "        Dense(len(unique_labels), activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=[AUC()])\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_resnet_model(image_size):\n",
    "    base_model = tf.keras.applications.ResNet50(include_top=False, weights='imagenet', input_shape=(image_size, image_size, 3), pooling=None)\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(base_model)\n",
    "    model.add(Conv2D(2048, (1, 1), activation=\"relu\")) # Transition Layer\n",
    "    model.add(GlobalAveragePooling2D())   \n",
    "    model.add(Dense(4, activation='softmax', name=\"predictions\"))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[AUC()])\n",
    "    return model\n",
    "\n",
    "def evaluate(model, images_val, labels_val, num_classes):\n",
    "    # Get the model's predictions (probabilities) for the validation set\n",
    "    probabilities = model.predict(images_val)\n",
    "\n",
    "    # Initialize an empty list to store the AUCs\n",
    "    auc_per_class = {}\n",
    "\n",
    "    # Calculate AUC for each class\n",
    "    for class_idx in range(num_classes):\n",
    "        true_labels = labels_val[:, class_idx]  # True labels for this class\n",
    "        pred_probs = probabilities[:, class_idx]  # Predicted probabilities for this class\n",
    "\n",
    "        # Calculate the AUC for this class\n",
    "        auc = sklearn.metrics.roc_auc_score(true_labels, pred_probs)\n",
    "        auc_per_class[labels_for_class[class_idx]] = auc        \n",
    "        \n",
    "    probabilities_transformed = probabilities.argmax(axis=1)\n",
    "    labels_val_transformed = labels_val.argmax(axis=1)\n",
    "\n",
    "    balanced_acc = sklearn.metrics.balanced_accuracy_score(labels_val_transformed, probabilities_transformed)\n",
    "    acc = sklearn.metrics.accuracy_score(labels_val_transformed, probabilities_transformed)\n",
    "\n",
    "    return auc_per_class, balanced_acc, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:08<00:00, 112.85it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 114.15it/s]\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SIZE = 224\n",
    "NUMBER_OF_IMAGES = 1000\n",
    "\n",
    "def normalize_image(img, label):\n",
    "    img = tf.cast(img, np.float32) / 255.0\n",
    "    # img -= np.array([123.68, 116.78, 103.94], dtype=np.float32)\n",
    "    return img, label\n",
    "\n",
    "images, labels = prepare_data(df[:NUMBER_OF_IMAGES], IMAGE_SIZE)\n",
    "images_test, labels_test = prepare_data(df[-(int(NUMBER_OF_IMAGES / 5)):], IMAGE_SIZE)\n",
    "\n",
    "\n",
    "################# only used if model requires 3 channels\n",
    "# images = np.repeat(images[..., np.newaxis], 3, axis=-1).reshape(-1, IMAGE_SIZE, IMAGE_SIZE, 3)\n",
    "# images_test = np.repeat(images_test[..., np.newaxis], 3, axis=-1).reshape(-1, IMAGE_SIZE, IMAGE_SIZE, 3)\n",
    "\n",
    "images_train, images_val, labels_train, labels_val = sklearn.model_selection.train_test_split(images, labels, random_state=42, test_size=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-27T20:46:04.081691Z",
     "start_time": "2025-02-27T20:45:53.353922Z"
    }
   },
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 21:46:17.726816: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n",
      "2025-02-27 21:46:17.726850: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2025-02-27 21:46:17.726861: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2025-02-27 21:46:17.726880: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-02-27 21:46:17.726890: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 21:46:18.725768: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_true [[0 0 0 1]\n",
      " [0 1 0 0]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]]\n",
      "y_pred [[0.329877734 0.194383487 0.237059191 0.238679573]\n",
      " [0.309258729 0.203050971 0.226483196 0.261207104]\n",
      " [0.341368735 0.197637409 0.222973764 0.238020122]\n",
      " [0.226382107 0.224006325 0.239933863 0.30967775]]\n",
      "positve_cases [0 1 0 0]\n",
      "negative_cases [1 0 1 1]\n",
      "count_positive 1\n",
      "count_negative 3\n",
      "total_samples 4\n",
      "beta_p 4\n",
      "beta_n 1.33333337\n",
      "y_true_binary [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]]\n",
      "y_pred_binary [[0.761320412 0.238679573]\n",
      " [0.738792896 0.261207104]\n",
      " [0.761979938 0.238020122]\n",
      " [0.69032228 0.30967775]]\n",
      "weighted_positive_loss 1.2109499\n",
      "weighted_negative_loss 5.38700676\n",
      "\u001B[1m  1/200\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m8:24\u001B[0m 3s/step - auc_1: 0.5312 - loss: 6.5980y_true [[0 0 1 0]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 1 0]]\n",
      "y_pred [[8.95633256e-09 1.91568915e-06 2.67647847e-05 0.999971271]\n",
      " [4.5898943e-10 2.27380454e-07 6.15224053e-06 0.999993563]\n",
      " [7.86941854e-08 1.62402484e-05 5.98963743e-05 0.999923825]\n",
      " [7.76286857e-09 3.67611256e-06 1.0463006e-09 0.999996305]]\n",
      "positve_cases [1 0 0 1]\n",
      "negative_cases [0 1 1 0]\n",
      "count_positive 2\n",
      "count_negative 2\n",
      "total_samples 4\n",
      "beta_p 2\n",
      "beta_n 2\n",
      "y_true_binary [[1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "y_pred_binary [[2.86894301e-05 0.999971271]\n",
      " [6.38008e-06 0.999993563]\n",
      " [7.6215314e-05 0.999923825]\n",
      " [3.68492169e-06 0.999996305]]\n",
      "weighted_positive_loss 45.8799744\n",
      "weighted_negative_loss 0.000164753073\n",
      "\u001B[1m  2/200\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m28s\u001B[0m 145ms/step - auc_1: 0.5820 - loss: 16.4185y_true [[0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 1 0]\n",
      " [1 0 0 0]]\n",
      "y_pred [[6.50442148e-15 6.14979179e-10 2.63641498e-10 1]\n",
      " [1.62919425e-17 4.31423619e-10 1.09424025e-09 1]\n",
      " [1.74943921e-06 7.77442377e-09 0.000164278 0.999833941]\n",
      " [1.57850885e-10 3.52401298e-06 2.90203644e-07 0.999996185]]\n",
      "positve_cases [0 0 1 1]\n",
      "negative_cases [1 1 0 0]\n",
      "count_positive 2\n",
      "count_negative 2\n",
      "total_samples 4\n",
      "beta_p 2\n",
      "beta_n 2\n",
      "y_true_binary [[0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]]\n",
      "y_pred_binary [[8.78627171e-10 1]\n",
      " [1.52566382e-09 1]\n",
      " [0.000166035214 0.999833941]\n",
      " [3.8143744e-06 0.999996185]]\n",
      "weighted_positive_loss 42.3071289\n",
      "weighted_negative_loss -4.7683713e-07\n",
      "\u001B[1m  3/200\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m27s\u001B[0m 138ms/step - auc_1: 0.6052 - loss: 21.4774y_true [[0 0 0 1]\n",
      " [0 0 1 0]\n",
      " [0 0 1 0]\n",
      " [0 1 0 0]]\n",
      "y_pred [[5.43433687e-10 0.13380684 0.00950460508 0.856688559]\n",
      " [2.10808562e-08 0.532487452 0.431227833 0.0362847298]\n",
      " [3.8187951e-15 3.85831117e-10 2.4534873e-07 0.999999762]\n",
      " [2.41043938e-11 2.56412168e-05 0.073599577 0.926374793]]\n",
      "positve_cases [0 1 1 1]\n",
      "negative_cases [1 0 0 0]\n",
      "count_positive 3\n",
      "count_negative 1\n",
      "total_samples 4\n",
      "beta_p 1.33333337\n",
      "beta_n 4\n",
      "y_true_binary [[0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]]\n",
      "y_pred_binary [[0.143311441 0.856688559]\n",
      " [0.963715315 0.0362847298]\n",
      " [2.45734554e-07 0.999999762]\n",
      " [0.0736252218 0.926374793]]\n",
      "weighted_positive_loss 23.3644295\n",
      "weighted_negative_loss 0.618722796\n",
      "\u001B[1m  4/200\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m25s\u001B[0m 133ms/step - auc_1: 0.6107 - loss: 23.5310y_true [[0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 1 0]]\n",
      "y_pred [[3.78424245e-12 2.02337577e-10 0.999991894 8.08440927e-06]\n",
      " [1.14502917e-13 0.000187932805 0.414547026 0.585265]\n",
      " [3.61826915e-13 2.91408156e-07 0.999839067 0.000160636104]\n",
      " [2.28345207e-11 0.0954056382 0.000570693868 0.904023647]]\n",
      "positve_cases [0 0 0 1]\n",
      "negative_cases [1 1 1 0]\n",
      "count_positive 1\n",
      "count_negative 3\n",
      "total_samples 4\n",
      "beta_p 4\n",
      "beta_n 1.33333337\n",
      "y_true_binary [[0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "y_pred_binary [[0.999991894 8.08440927e-06]\n",
      " [0.41473496 0.585265]\n",
      " [0.999839365 0.000160636104]\n",
      " [0.0959763303 0.904023647]]\n",
      "weighted_positive_loss 9.3746109\n",
      "weighted_negative_loss 27.9796219\n",
      "\u001B[1m  5/200\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m25s\u001B[0m 130ms/step - auc_1: 0.6044 - loss: 25.0697y_true [[0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 1 0 0]\n",
      " [0 0 1 0]]\n",
      "y_pred [[1.06548918e-14 0.147573113 0.782289684 0.0701372102]\n",
      " [5.73725149e-13 1.99402912e-08 0.0497303903 0.950269639]\n",
      " [1.44329967e-16 1.08202255e-06 0.371925831 0.628073096]\n",
      " [1.3486578e-14 1.22352386e-08 0.96665132 0.0333486237]]\n",
      "positve_cases [0 0 1 1]\n",
      "negative_cases [1 1 0 0]\n",
      "count_positive 2\n",
      "count_negative 2\n",
      "total_samples 4\n",
      "beta_p 2\n",
      "beta_n 2\n",
      "y_true_binary [[0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]]\n",
      "y_pred_binary [[0.929862797 0.0701372102]\n",
      " [0.0497304089 0.950269639]\n",
      " [0.371926904 0.628073096]\n",
      " [0.96665132 0.0333486237]]\n",
      "weighted_positive_loss 2.04595\n",
      "weighted_negative_loss 5.41662\n",
      "\u001B[1m  6/200\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m24s\u001B[0m 129ms/step - auc_1: 0.6030 - loss: 25.4355y_true [[0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 1 0]]\n",
      "y_pred [[0.000128154483 0.0418399572 0.957023203 0.00100871851]\n",
      " [2.32762486e-12 0.991461396 0.00849127769 4.73363143e-05]\n",
      " [6.10340944e-12 0.0432402268 0.48785612 0.46890372]\n",
      " [1.31861475e-11 0.0213337261 0.978314579 0.000351718365]]\n",
      "positve_cases [0 0 0 1]\n",
      "negative_cases [1 1 1 0]\n",
      "count_positive 1\n",
      "count_negative 3\n",
      "total_samples 4\n",
      "beta_p 4\n",
      "beta_n 1.33333337\n",
      "y_true_binary [[0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "y_pred_binary [[0.998991311 0.00100871851]\n",
      " [0.999952674 4.73363143e-05]\n",
      " [0.531096339 0.46890372]\n",
      " [0.999648333 0.000351718365]]\n",
      "weighted_positive_loss 0.00140644\n",
      "weighted_negative_loss 23.4832745\n",
      "\u001B[1m  7/200\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m24s\u001B[0m 127ms/step - auc_1: 0.6010 - loss: 25.6196y_true [[0 0 0 1]\n",
      " [0 0 1 0]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]]\n",
      "y_pred [[6.45680848e-05 2.54036e-06 0.999026537 0.000906397356]\n",
      " [1.31483435e-11 0.124798581 0.875068605 0.000132802088]\n",
      " [1.50265014e-13 0.958395422 0.037941616 0.00366292801]\n",
      " [1.16085493e-14 0.973295629 0.01192059 0.0147837577]]\n",
      "positve_cases [0 1 0 0]\n",
      "negative_cases [1 0 1 1]\n",
      "count_positive 1\n",
      "count_negative 3\n",
      "total_samples 4\n",
      "beta_p 4\n",
      "beta_n 1.33333337\n",
      "y_true_binary [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]]\n",
      "y_pred_binary [[0.999093652 0.000906397356]\n",
      " [0.999867201 0.000132802088]\n",
      " [0.996337056 0.00366292801]\n",
      " [0.9852162 0.0147837577]]\n",
      "weighted_positive_loss 0.000530755\n",
      "weighted_negative_loss 22.439476\n",
      "\u001B[1m  8/200\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m24s\u001B[0m 126ms/step - auc_1: 0.5975 - loss: 25.6907y_true [[0 0 0 1]\n",
      " [0 0 1 0]\n",
      " [0 0 0 1]\n",
      " [0 1 0 0]]\n",
      "y_pred [[5.86220186e-20 2.79596156e-06 1.11634768e-06 0.999996185]\n",
      " [9.74115672e-15 0.000100967984 0.999760091 0.00013889416]\n",
      " [7.29745257e-13 0.999422669 0.000467701437 0.000109572007]\n",
      " [1.81678128e-09 0.197393671 0.748695731 0.0539105721]]\n",
      "positve_cases [0 1 0 1]\n",
      "negative_cases [1 0 1 0]\n",
      "count_positive 2\n",
      "count_negative 2\n",
      "total_samples 4\n",
      "beta_p 2\n",
      "beta_n 2\n",
      "y_true_binary [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "y_pred_binary [[3.91230924e-06 0.999996185]\n",
      " [0.999861062 0.00013889416]\n",
      " [0.999890387 0.000109572007]\n",
      " [0.946089387 0.0539105721]]\n",
      "weighted_positive_loss 0.111113861\n",
      "weighted_negative_loss 18.2360401\n",
      "\u001B[1m  9/200\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m23s\u001B[0m 126ms/step - auc_1: 0.5975 - loss: 25.6493y_true [[0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [1 0 0 0]\n",
      " [0 0 0 1]]\n",
      "y_pred [[2.97906163e-14 0.0361082144 0.000339555467 0.963552296]\n",
      " [1.656771e-24 1 1.54705748e-09 1.9049803e-12]\n",
      " [2.33434608e-19 6.97247e-05 0.999566495 0.000363741856]\n",
      " [1.53598705e-13 0.987642765 0.00399211189 0.00836518407]]\n",
      "positve_cases [0 0 1 0]\n",
      "negative_cases [1 1 0 1]\n",
      "count_positive 1\n",
      "count_negative 3\n",
      "total_samples 4\n",
      "beta_p 4\n",
      "beta_n 1.33333337\n",
      "y_true_binary [[0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]]\n",
      "y_pred_binary [[0.0364477709 0.963552296]\n",
      " [1 1.9049803e-12]\n",
      " [0.999636233 0.000363741856]\n",
      " [0.991634905 0.00836518407]]\n",
      "weighted_positive_loss 0.00145485625\n",
      "weighted_negative_loss 27.9184933\n",
      "\u001B[1m 10/200\u001B[0m \u001B[32m━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m24s\u001B[0m 127ms/step - auc_1: 0.5966 - loss: 25.6421y_true [[0 0 1 0]\n",
      " [0 0 0 1]\n",
      " [0 0 1 0]\n",
      " [1 0 0 0]]\n",
      "y_pred [[6.28682355e-14 0.382739633 0.0596531183 0.557607293]\n",
      " [8.95176711e-33 1 3.48432705e-09 1.60072097e-13]\n",
      " [4.85678864e-22 1.10370479e-15 0.981082678 0.0189173706]\n",
      " [1.1229962e-14 3.92708479e-08 0.999998689 1.22302197e-06]]\n",
      "positve_cases [1 0 1 1]\n",
      "negative_cases [0 1 0 0]\n",
      "count_positive 3\n",
      "count_negative 1\n",
      "total_samples 4\n",
      "beta_p 1.33333337\n",
      "beta_n 4\n",
      "y_true_binary [[1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]]\n",
      "y_pred_binary [[0.442392766 0.557607293]\n",
      " [1 1.60072097e-13]\n",
      " [0.981082678 0.0189173706]\n",
      " [0.999998748 1.22302197e-06]]\n",
      "weighted_positive_loss 1.11287546\n",
      "weighted_negative_loss 64.472374\n",
      "\u001B[1m 11/200\u001B[0m \u001B[32m━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m23s\u001B[0m 127ms/step - auc_1: 0.5952 - loss: 25.9669y_true [[0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 1 0 0]]\n",
      "y_pred [[2.61190131e-13 0.0100545073 0.000156350608 0.989789188]\n",
      " [4.79744241e-17 0.19731918 0.00156747468 0.801113367]\n",
      " [1.69108449e-20 3.36709149e-06 3.74058836e-05 0.99995923]\n",
      " [2.8517039e-22 7.85755958e-07 7.53825225e-05 0.999923825]]\n",
      "positve_cases [0 0 0 1]\n",
      "negative_cases [1 1 1 0]\n",
      "count_positive 1\n",
      "count_negative 3\n",
      "total_samples 4\n",
      "beta_p 4\n",
      "beta_n 1.33333337\n",
      "y_true_binary [[0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "y_pred_binary [[0.0102108577 0.989789188]\n",
      " [0.198886648 0.801113367]\n",
      " [4.07729749e-05 0.99995923]\n",
      " [7.6168275e-05 0.999923825]]\n",
      "weighted_positive_loss 37.9250145\n",
      "weighted_negative_loss 0.309408665\n",
      "\u001B[1m 12/200\u001B[0m \u001B[32m━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m23s\u001B[0m 127ms/step - auc_1: 0.5958 - loss: 26.3002y_true [[0 0 0 1]\n",
      " [0 1 0 0]\n",
      " [0 0 0 1]\n",
      " [0 0 1 0]]\n",
      "y_pred [[8.93625e-27 0.9496544 1.77547987e-09 0.0503455326]\n",
      " [3.10266057e-21 0.000603484921 0.000140203629 0.999256313]\n",
      " [2.92216354e-21 1.45623244e-05 0.999985456 2.50585277e-08]\n",
      " [5.9589435e-32 0.000277137238 6.17127668e-11 0.999722898]]\n",
      "positve_cases [0 1 0 1]\n",
      "negative_cases [1 0 1 0]\n",
      "count_positive 2\n",
      "count_negative 2\n",
      "total_samples 4\n",
      "beta_p 2\n",
      "beta_n 2\n",
      "y_true_binary [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "y_pred_binary [[0.9496544 0.0503455326]\n",
      " [0.00074368855 0.999256313]\n",
      " [1 2.50585277e-08]\n",
      " [0.000277137297 0.999722898]]\n",
      "weighted_positive_loss 30.7887802\n",
      "weighted_negative_loss 37.766655\n",
      "\u001B[1m 13/200\u001B[0m \u001B[32m━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m23s\u001B[0m 128ms/step - auc_1: 0.5948 - loss: 26.8105y_true [[0 0 1 0]\n",
      " [0 0 0 1]\n",
      " [0 1 0 0]\n",
      " [0 0 0 1]]\n",
      "y_pred [[5.34336512e-25 0.970068574 0.0299311765 2.36567303e-07]\n",
      " [6.62810388e-12 0.000164872559 0.987761617 0.0120734749]\n",
      " [1.98449777e-21 0.00581787247 1.94166759e-07 0.994181931]\n",
      " [5.93211701e-29 0.000476266112 5.89199384e-11 0.999523759]]\n",
      "positve_cases [1 0 1 0]\n",
      "negative_cases [0 1 0 1]\n",
      "count_positive 2\n",
      "count_negative 2\n",
      "total_samples 4\n",
      "beta_p 2\n",
      "beta_n 2\n",
      "y_true_binary [[1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]]\n",
      "y_pred_binary [[0.999999762 2.36567303e-07]\n",
      " [0.987926483 0.0120734749]\n",
      " [0.00581806665 0.994181931]\n",
      " [0.00047626617 0.999523759]]\n",
      "weighted_positive_loss 10.29354\n",
      "weighted_negative_loss 8.83442497\n",
      "\u001B[1m 14/200\u001B[0m \u001B[32m━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m23s\u001B[0m 128ms/step - auc_1: 0.5947 - loss: 27.1775y_true [[0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 1 0]]\n",
      "y_pred [[5.64898806e-21 4.33921832e-09 6.54716246e-08 0.999999881]\n",
      " [1.07300819e-13 0.0521294214 0.140205428 0.80766511]\n",
      " [2.00370363e-12 0.00915418752 0.000945284555 0.989900529]\n",
      " [1.11748447e-12 3.90325239e-09 0.996766925 0.00323312357]]\n",
      "positve_cases [0 0 0 1]\n",
      "negative_cases [1 1 1 0]\n",
      "count_positive 1\n",
      "count_negative 3\n",
      "total_samples 4\n",
      "beta_p 4\n",
      "beta_n 1.33333337\n",
      "y_true_binary [[0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "y_pred_binary [[6.98108451e-08 0.999999881]\n",
      " [0.192334846 0.80766511]\n",
      " [0.0100994725 0.989900529]\n",
      " [0.996766925 0.00323312357]]\n",
      "weighted_positive_loss 0.0129527729\n",
      "weighted_negative_loss 0.298344433\n",
      "\u001B[1m 15/200\u001B[0m \u001B[32m━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m23s\u001B[0m 128ms/step - auc_1: 0.5965 - loss: 27.3550y_true [[0 0 0 1]\n",
      " [0 0 1 0]\n",
      " [0 0 0 1]\n",
      " [0 0 1 0]]\n",
      "y_pred [[3.46463617e-19 4.95325821e-06 8.35309655e-10 0.999995]\n",
      " [2.41483168e-31 0.353737712 5.50374819e-15 0.646262288]\n",
      " [1.4222931e-26 0.00263976608 1.77318202e-10 0.99736017]\n",
      " [9.84560384e-20 2.44167552e-07 0.857745528 0.142254189]]\n",
      "positve_cases [0 1 0 1]\n",
      "negative_cases [1 0 1 0]\n",
      "count_positive 2\n",
      "count_negative 2\n",
      "total_samples 4\n",
      "beta_p 2\n",
      "beta_n 2\n",
      "y_true_binary [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "y_pred_binary [[4.95409358e-06 0.999995]\n",
      " [0.353737712 0.646262288]\n",
      " [0.00263976632 0.99736017]\n",
      " [0.857745767 0.142254189]]\n",
      "weighted_positive_loss 2.38529348\n",
      "weighted_negative_loss 0.00529617723\n",
      "\u001B[1m 16/200\u001B[0m \u001B[32m━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m23s\u001B[0m 127ms/step - auc_1: 0.5991 - loss: 27.4031y_true [[0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]]\n",
      "y_pred [[7.97078059e-23 1 1.66397252e-09 2.25594432e-10]\n",
      " [1.44267367e-21 0.0115263 0.639128 0.349345624]\n",
      " [8.90999842e-26 0.981976151 1.29033679e-05 0.0180109739]\n",
      " [6.23186622e-07 4.21530458e-05 7.83137511e-05 0.999878883]]\n",
      "positve_cases [0 0 0 0]\n",
      "negative_cases [1 1 1 1]\n",
      "count_positive 0\n",
      "count_negative 4\n",
      "total_samples 4\n",
      "beta_p 1e+07\n",
      "beta_n 1\n",
      "y_true_binary [[0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]]\n",
      "y_pred_binary [[1 2.25594432e-10]\n",
      " [0.650654316 0.349345624]\n",
      " [0.981989 0.0180109739]\n",
      " [0.000121089979 0.999878883]]\n",
      "weighted_positive_loss 0\n",
      "weighted_negative_loss 21.1844254\n",
      "\u001B[1m 17/200\u001B[0m \u001B[32m━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m23s\u001B[0m 127ms/step - auc_1: 0.6014 - loss: 27.4214y_true [[0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 1 0 0]]\n",
      "y_pred [[1.42039347e-15 0.089697428 0.00485168444 0.905450821]\n",
      " [2.57397514e-19 4.29953406e-09 1 2.26420482e-09]\n",
      " [4.47071508e-20 1.21561275e-10 3.66918197e-11 1]\n",
      " [9.42286143e-16 1.8310817e-06 3.54685392e-09 0.999998212]]\n",
      "positve_cases [0 0 0 1]\n",
      "negative_cases [1 1 1 0]\n",
      "count_positive 1\n",
      "count_negative 3\n",
      "total_samples 4\n",
      "beta_p 4\n",
      "beta_n 1.33333337\n",
      "y_true_binary [[0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "y_pred_binary [[0.094549112 0.905450821]\n",
      " [1 2.26420482e-09]\n",
      " [1.58253091e-10 1]\n",
      " [1.83462851e-06 0.999998212]]\n",
      "weighted_positive_loss 52.6223793\n",
      "weighted_negative_loss 21.5933723\n",
      "\u001B[1m 18/200\u001B[0m \u001B[32m━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m23s\u001B[0m 127ms/step - auc_1: 0.6035 - loss: 27.5813y_true [[0 0 0 1]\n",
      " [0 0 1 0]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]]\n",
      "y_pred [[0 0.999991059 9.19698779e-12 8.99068709e-06]\n",
      " [2.86567194e-17 0.00038147188 1.42403014e-05 0.999604285]\n",
      " [2.09209329e-17 0.0440788902 0.00177312945 0.954147935]\n",
      " [9.09758e-14 1.05298591e-06 0.998321235 0.00167767343]]\n",
      "positve_cases [0 1 0 0]\n",
      "negative_cases [1 0 1 1]\n",
      "count_positive 1\n",
      "count_negative 3\n",
      "total_samples 4\n",
      "beta_p 4\n",
      "beta_n 1.33333337\n",
      "y_true_binary [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]]\n",
      "y_pred_binary [[0.999991059 8.99068709e-06]\n",
      " [0.000395712181 0.999604285]\n",
      " [0.0458520204 0.954147935]\n",
      " [0.998322308 0.00167767343]]\n",
      "weighted_positive_loss 31.3382835\n",
      "weighted_negative_loss 24.0606461\n",
      "\u001B[1m 19/200\u001B[0m \u001B[32m━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m22s\u001B[0m 127ms/step - auc_1: 0.6048 - loss: 27.7939y_true [[0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 1 0 0]\n",
      " [0 0 0 1]]\n",
      "y_pred [[1.39186032e-18 0.286537617 0.165530458 0.54793191]\n",
      " [6.3217026e-13 1.13809767e-06 0.0579687208 0.942030132]\n",
      " [2.6465545e-11 0.995741904 1.10877302e-07 0.00425789226]\n",
      " [3.54735303e-36 1 8.76498513e-15 1.6224313e-13]]\n",
      "positve_cases [0 0 1 0]\n",
      "negative_cases [1 1 0 1]\n",
      "count_positive 1\n",
      "count_negative 3\n",
      "total_samples 4\n",
      "beta_p 4\n",
      "beta_n 1.33333337\n",
      "y_true_binary [[0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]]\n",
      "y_pred_binary [[0.45206809 0.54793191]\n",
      " [0.0579698607 0.942030132]\n",
      " [0.995742 0.00425789226]\n",
      " [1 1.6224313e-13]]\n",
      "weighted_positive_loss 0.0170677938\n",
      "weighted_negative_loss 22.3725548\n",
      "\u001B[1m 20/200\u001B[0m \u001B[32m━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m22s\u001B[0m 127ms/step - auc_1: 0.6063 - loss: 27.9621y_true [[0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]]\n",
      "y_pred [[1.25430619e-24 3.8059224e-07 0.0015606567 0.998439]\n",
      " [4.06106472e-17 0.0669136718 0.0565689 0.876517415]\n",
      " [5.4998519e-22 0.452791482 0.013866256 0.533342242]\n",
      " [3.83550176e-28 7.1884676e-09 1.64461262e-08 1]]\n",
      "positve_cases [0 0 0 0]\n",
      "negative_cases [1 1 1 1]\n",
      "count_positive 0\n",
      "count_negative 4\n",
      "total_samples 4\n",
      "beta_p 1e+07\n",
      "beta_n 1\n",
      "y_true_binary [[0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]]\n",
      "y_pred_binary [[0.00156103726 0.998439]\n",
      " [0.12348257 0.876517415]\n",
      " [0.466657728 0.533342242]\n",
      " [2.36345947e-08 1]]\n",
      "weighted_positive_loss 0\n",
      "weighted_negative_loss 0.761952221\n",
      "\u001B[1m 21/200\u001B[0m \u001B[32m━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m22s\u001B[0m 127ms/step - auc_1: 0.6084 - loss: 28.0454y_true [[0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [1 0 0 0]\n",
      " [0 1 0 0]]\n",
      "y_pred [[1.23733915e-18 0.204748183 4.02786027e-05 0.795211554]\n",
      " [7.02847547e-20 0.844982386 1.42601682e-08 0.155017614]\n",
      " [4.11126341e-12 0.889627278 0.00633236952 0.104040347]\n",
      " [3.57039952e-30 4.47372895e-06 1.71789992e-15 0.99999547]]\n",
      "positve_cases [0 0 1 1]\n",
      "negative_cases [1 1 0 0]\n",
      "count_positive 2\n",
      "count_negative 2\n",
      "total_samples 4\n",
      "beta_p 2\n",
      "beta_n 2\n",
      "y_true_binary [[0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]]\n",
      "y_pred_binary [[0.204788461 0.795211554]\n",
      " [0.844982386 0.155017614]\n",
      " [0.895959675 0.104040347]\n",
      " [4.47372895e-06 0.99999547]]\n",
      "weighted_positive_loss 24.8100834\n",
      "weighted_negative_loss 4.18672562\n",
      "\u001B[1m 22/200\u001B[0m \u001B[32m━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m22s\u001B[0m 126ms/step - auc_1: 0.6101 - loss: 28.1196y_true [[0 1 0 0]\n",
      " [0 0 1 0]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]]\n",
      "y_pred [[1.49373271e-13 4.36512579e-07 0.00540210912 0.994597435]\n",
      " [1.09456251e-31 0.193519026 8.30858369e-07 0.80648011]\n",
      " [8.54785487e-28 1 5.97569032e-13 4.64330654e-08]\n",
      " [4.40881695e-26 1 3.40698498e-12 4.11549941e-08]]\n",
      "positve_cases [1 1 0 0]\n",
      "negative_cases [0 0 1 1]\n",
      "count_positive 2\n",
      "count_negative 2\n",
      "total_samples 4\n",
      "beta_p 2\n",
      "beta_n 2\n",
      "y_true_binary [[1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]]\n",
      "y_pred_binary [[0.00540254544 0.994597435]\n",
      " [0.193519861 0.80648011]\n",
      " [1 4.64330654e-08]\n",
      " [1 4.11549941e-08]]\n",
      "weighted_positive_loss 13.7264824\n",
      "weighted_negative_loss 63.0202103\n",
      "\u001B[1m 23/200\u001B[0m \u001B[32m━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m22s\u001B[0m 126ms/step - auc_1: 0.6108 - loss: 28.2764y_true [[0 0 1 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]]\n",
      "y_pred [[3.23129602e-15 7.41522963e-05 0.0137403104 0.986185491]\n",
      " [3.20109709e-14 3.27300398e-07 0.998363912 0.00163571036]\n",
      " [7.95794637e-14 0.999999523 3.0551908e-07 1.01292272e-07]\n",
      " [1.50403927e-13 0.0708654225 0.0334958434 0.895638764]]\n",
      "positve_cases [1 1 0 0]\n",
      "negative_cases [0 0 1 1]\n",
      "count_positive 2\n",
      "count_negative 2\n",
      "total_samples 4\n",
      "beta_p 2\n",
      "beta_n 2\n",
      "y_true_binary [[1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]]\n",
      "y_pred_binary [[0.0138144623 0.986185491]\n",
      " [0.99836421 0.00163571036]\n",
      " [0.999999821 1.01292272e-07]\n",
      " [0.104361266 0.895638764]]\n",
      "weighted_positive_loss 8.56733799\n",
      "weighted_negative_loss 31.0574512\n",
      "\u001B[1m 24/200\u001B[0m \u001B[32m━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m22s\u001B[0m 126ms/step - auc_1: 0.6112 - loss: 28.4338y_true [[0 0 1 0]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]]\n",
      "y_pred [[4.32705384e-20 1 1.5631461e-12 9.24795917e-12]\n",
      " [1.47099968e-20 0.99656564 0.00343441986 4.66258534e-08]\n",
      " [2.73779681e-18 0.967244864 2.49603727e-10 0.0327551253]\n",
      " [8.74670623e-18 0.034549389 0.0102600185 0.955190599]]\n",
      "positve_cases [1 0 0 0]\n",
      "negative_cases [0 1 1 1]\n",
      "count_positive 1\n",
      "count_negative 3\n",
      "total_samples 4\n",
      "beta_p 4\n",
      "beta_n 1.33333337\n",
      "y_true_binary [[1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]]\n",
      "y_pred_binary [[1 9.24795917e-12]\n",
      " [1.00000012 4.66258534e-08]\n",
      " [0.967244864 0.0327551253]\n",
      " [0.0448094085 0.955190599]]\n",
      "weighted_positive_loss -4.7683713e-07\n",
      "weighted_negative_loss 25.5998917\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 22\u001B[0m\n\u001B[1;32m     18\u001B[0m class_weights \u001B[38;5;241m=\u001B[39m {i: class_weights[i] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(class_weights))}\n\u001B[1;32m     21\u001B[0m early_stopping \u001B[38;5;241m=\u001B[39m EarlyStopping(monitor\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval_loss\u001B[39m\u001B[38;5;124m'\u001B[39m, patience\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m, restore_best_weights\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m---> 22\u001B[0m history \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mBATCH_SIZE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msteps_per_epoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mimages_train\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mBATCH_SIZE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mimages_val\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43mBATCH_SIZE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mearly_stopping\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;66;03m#, class_weight=class_weights)\u001B[39;00m\n\u001B[1;32m     24\u001B[0m evals \u001B[38;5;241m=\u001B[39m evaluate(model, images_test, labels_test, \u001B[38;5;28mlen\u001B[39m(unique_labels))\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28mprint\u001B[39m(evals)\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/medicaldeeplearning-6tfcUi4t-py3.10/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    115\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    116\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 117\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    118\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    119\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/medicaldeeplearning-6tfcUi4t-py3.10/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:371\u001B[0m, in \u001B[0;36mTensorFlowTrainer.fit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001B[0m\n\u001B[1;32m    369\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m step, iterator \u001B[38;5;129;01min\u001B[39;00m epoch_iterator:\n\u001B[1;32m    370\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_begin(step)\n\u001B[0;32m--> 371\u001B[0m     logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    372\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_end(step, logs)\n\u001B[1;32m    373\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstop_training:\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/medicaldeeplearning-6tfcUi4t-py3.10/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:219\u001B[0m, in \u001B[0;36mTensorFlowTrainer._make_function.<locals>.function\u001B[0;34m(iterator)\u001B[0m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mfunction\u001B[39m(iterator):\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\n\u001B[1;32m    217\u001B[0m         iterator, (tf\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mIterator, tf\u001B[38;5;241m.\u001B[39mdistribute\u001B[38;5;241m.\u001B[39mDistributedIterator)\n\u001B[1;32m    218\u001B[0m     ):\n\u001B[0;32m--> 219\u001B[0m         opt_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmulti_step_on_iterator\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    220\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m opt_outputs\u001B[38;5;241m.\u001B[39mhas_value():\n\u001B[1;32m    221\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/medicaldeeplearning-6tfcUi4t-py3.10/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/medicaldeeplearning-6tfcUi4t-py3.10/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    830\u001B[0m compiler \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnonXla\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    832\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile):\n\u001B[0;32m--> 833\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    835\u001B[0m new_tracing_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperimental_get_tracing_count()\n\u001B[1;32m    836\u001B[0m without_tracing \u001B[38;5;241m=\u001B[39m (tracing_count \u001B[38;5;241m==\u001B[39m new_tracing_count)\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/medicaldeeplearning-6tfcUi4t-py3.10/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001B[0m, in \u001B[0;36mFunction._call\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    875\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[1;32m    876\u001B[0m \u001B[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001B[39;00m\n\u001B[1;32m    877\u001B[0m \u001B[38;5;66;03m# run the first trace but we should fail if variables are created.\u001B[39;00m\n\u001B[0;32m--> 878\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[43mtracing_compilation\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_function\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    879\u001B[0m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_variable_creation_config\u001B[49m\n\u001B[1;32m    880\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    881\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_created_variables:\n\u001B[1;32m    882\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCreating variables on a non-first call to a function\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    883\u001B[0m                    \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m decorated with tf.function.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/medicaldeeplearning-6tfcUi4t-py3.10/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001B[0m, in \u001B[0;36mcall_function\u001B[0;34m(args, kwargs, tracing_options)\u001B[0m\n\u001B[1;32m    137\u001B[0m bound_args \u001B[38;5;241m=\u001B[39m function\u001B[38;5;241m.\u001B[39mfunction_type\u001B[38;5;241m.\u001B[39mbind(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    138\u001B[0m flat_inputs \u001B[38;5;241m=\u001B[39m function\u001B[38;5;241m.\u001B[39mfunction_type\u001B[38;5;241m.\u001B[39munpack_inputs(bound_args)\n\u001B[0;32m--> 139\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_flat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# pylint: disable=protected-access\u001B[39;49;00m\n\u001B[1;32m    140\u001B[0m \u001B[43m    \u001B[49m\u001B[43mflat_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcaptured_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfunction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcaptured_inputs\u001B[49m\n\u001B[1;32m    141\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/medicaldeeplearning-6tfcUi4t-py3.10/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001B[0m, in \u001B[0;36mConcreteFunction._call_flat\u001B[0;34m(self, tensor_inputs, captured_inputs)\u001B[0m\n\u001B[1;32m   1318\u001B[0m possible_gradient_type \u001B[38;5;241m=\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPossibleTapeGradientTypes(args)\n\u001B[1;32m   1319\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (possible_gradient_type \u001B[38;5;241m==\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001B[1;32m   1320\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m executing_eagerly):\n\u001B[1;32m   1321\u001B[0m   \u001B[38;5;66;03m# No tape is watching; skip to running the function.\u001B[39;00m\n\u001B[0;32m-> 1322\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inference_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_preflattened\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1323\u001B[0m forward_backward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_select_forward_and_backward_functions(\n\u001B[1;32m   1324\u001B[0m     args,\n\u001B[1;32m   1325\u001B[0m     possible_gradient_type,\n\u001B[1;32m   1326\u001B[0m     executing_eagerly)\n\u001B[1;32m   1327\u001B[0m forward_function, args_with_tangents \u001B[38;5;241m=\u001B[39m forward_backward\u001B[38;5;241m.\u001B[39mforward()\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/medicaldeeplearning-6tfcUi4t-py3.10/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001B[0m, in \u001B[0;36mAtomicFunction.call_preflattened\u001B[0;34m(self, args)\u001B[0m\n\u001B[1;32m    214\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mcall_preflattened\u001B[39m(\u001B[38;5;28mself\u001B[39m, args: Sequence[core\u001B[38;5;241m.\u001B[39mTensor]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    215\u001B[0m \u001B[38;5;250m  \u001B[39m\u001B[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 216\u001B[0m   flat_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_flat\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    217\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunction_type\u001B[38;5;241m.\u001B[39mpack_output(flat_outputs)\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/medicaldeeplearning-6tfcUi4t-py3.10/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001B[0m, in \u001B[0;36mAtomicFunction.call_flat\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m    249\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m record\u001B[38;5;241m.\u001B[39mstop_recording():\n\u001B[1;32m    250\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_bound_context\u001B[38;5;241m.\u001B[39mexecuting_eagerly():\n\u001B[0;32m--> 251\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_bound_context\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_function\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    252\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    253\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    254\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunction_type\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflat_outputs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    255\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    256\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    257\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m make_call_op_in_graph(\n\u001B[1;32m    258\u001B[0m         \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    259\u001B[0m         \u001B[38;5;28mlist\u001B[39m(args),\n\u001B[1;32m    260\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_bound_context\u001B[38;5;241m.\u001B[39mfunction_call_options\u001B[38;5;241m.\u001B[39mas_attrs(),\n\u001B[1;32m    261\u001B[0m     )\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/medicaldeeplearning-6tfcUi4t-py3.10/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1500\u001B[0m, in \u001B[0;36mContext.call_function\u001B[0;34m(self, name, tensor_inputs, num_outputs)\u001B[0m\n\u001B[1;32m   1498\u001B[0m cancellation_context \u001B[38;5;241m=\u001B[39m cancellation\u001B[38;5;241m.\u001B[39mcontext()\n\u001B[1;32m   1499\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cancellation_context \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1500\u001B[0m   outputs \u001B[38;5;241m=\u001B[39m \u001B[43mexecute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1501\u001B[0m \u001B[43m      \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mutf-8\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[43m      \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1503\u001B[0m \u001B[43m      \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtensor_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1504\u001B[0m \u001B[43m      \u001B[49m\u001B[43mattrs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1505\u001B[0m \u001B[43m      \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1506\u001B[0m \u001B[43m  \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1507\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1508\u001B[0m   outputs \u001B[38;5;241m=\u001B[39m execute\u001B[38;5;241m.\u001B[39mexecute_with_cancellation(\n\u001B[1;32m   1509\u001B[0m       name\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m   1510\u001B[0m       num_outputs\u001B[38;5;241m=\u001B[39mnum_outputs,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1514\u001B[0m       cancellation_manager\u001B[38;5;241m=\u001B[39mcancellation_context,\n\u001B[1;32m   1515\u001B[0m   )\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/medicaldeeplearning-6tfcUi4t-py3.10/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001B[0m, in \u001B[0;36mquick_execute\u001B[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     52\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[0;32m---> 53\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_Execute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     54\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     55\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     56\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 4\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((images_train, labels_train))\n",
    "dataset = dataset.map(normalize_image)\n",
    "dataset = dataset.shuffle(buffer_size=100)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset = dataset.repeat()\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((images_val, labels_val))\n",
    "val_dataset = val_dataset.map(normalize_image)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=False)\n",
    "    \n",
    "model = create_model_1(IMAGE_SIZE)\n",
    "    \n",
    "model.compile(optimizer=Adam(learning_rate=1e-4), loss=weighted_cross_entropy_loss, metrics=[AUC()])\n",
    "\n",
    "class_weights = sklearn.utils.class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(labels_train.argmax(axis=1)), y=labels_train.argmax(axis=1))\n",
    "class_weights = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "history = model.fit(dataset, epochs=20, batch_size=BATCH_SIZE, steps_per_epoch=len(images_train) // BATCH_SIZE, validation_data=val_dataset, validation_steps=len(images_val)//BATCH_SIZE, callbacks=[early_stopping], verbose=1, class_weight=class_weights)\n",
    "\n",
    "evals = evaluate(model, images_test, labels_test, len(unique_labels))\n",
    "print(evals)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-27T20:46:24.607155Z",
     "start_time": "2025-02-27T20:46:17.716400Z"
    }
   },
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "pass"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-27T20:21:31.003041Z",
     "start_time": "2025-02-27T20:21:30.998941Z"
    }
   },
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "font_size = 16\n",
    "\n",
    "plt.plot(history.history['loss'], label=\"Loss\")\n",
    "plt.plot(history.history['val_loss'], label=\"Validation Loss\")\n",
    "\n",
    "plt.xlabel(\"Epochs\", fontsize=font_size)\n",
    "plt.grid(color='grey', linestyle='-', linewidth=0.25, alpha=0.5)\n",
    "plt.legend(fontsize=font_size, loc=\"center right\")\n",
    "\n",
    "plt.savefig(\"evals/history_256px_1000samples_model3.pdf\", bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Resolution\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "NUMBER_OF_IMAGES = 1000\n",
    "evaluations = {}\n",
    "\n",
    "def normalize_image(img, label):\n",
    "    img = tf.cast(img, np.float32) / 255.0\n",
    "    return img, label\n",
    "\n",
    "\n",
    "for ev_image_size in [50, 100, 150, 200, 250, 300, 350, 400]:\n",
    "    \n",
    "    images, labels = prepare_data(df[:NUMBER_OF_IMAGES], ev_image_size)\n",
    "    images_test, labels_test = prepare_data(df[-(int(NUMBER_OF_IMAGES / 5)):], ev_image_size)\n",
    "\n",
    "    images_train, images_val, labels_train, labels_val = sklearn.model_selection.train_test_split(images, labels, random_state=42, test_size=0.2)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images_train, labels_train))\n",
    "    dataset = dataset.map(normalize_image)\n",
    "    dataset = dataset.shuffle(buffer_size=100)\n",
    "    dataset = dataset.batch(BATCH_SIZE, drop_remainder=False)\n",
    "    dataset = dataset.repeat()\n",
    "    \n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((images_val, labels_val))\n",
    "    val_dataset = val_dataset.map(normalize_image)\n",
    "    val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=False)\n",
    "        \n",
    "    model = create_model_3(ev_image_size)\n",
    "        \n",
    "    class_weights = sklearn.utils.class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(labels_train.argmax(axis=1)), y=labels_train.argmax(axis=1))\n",
    "    class_weights = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    model.fit(dataset, epochs=20, batch_size=BATCH_SIZE, steps_per_epoch=len(images_train) // BATCH_SIZE, validation_data=val_dataset, validation_steps=len(images_val)//BATCH_SIZE, callbacks=[early_stopping], class_weight=class_weights, verbose=2)\n",
    "    \n",
    "    evals = evaluate(model, images_test, labels_test, len(unique_labels))\n",
    "    \n",
    "    evaluations[ev_image_size] = evals\n",
    "    print()\n",
    "    print(evals)\n",
    "    print()\n",
    "    \n",
    "print(evaluations)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 877/877 [00:07<00:00, 111.86it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 43\u001B[0m\n\u001B[1;32m     40\u001B[0m class_weights \u001B[38;5;241m=\u001B[39m sklearn\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mclass_weight\u001B[38;5;241m.\u001B[39mcompute_class_weight(class_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbalanced\u001B[39m\u001B[38;5;124m'\u001B[39m, classes\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39munique(labels_train\u001B[38;5;241m.\u001B[39margmax(axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)), y\u001B[38;5;241m=\u001B[39mlabels_train\u001B[38;5;241m.\u001B[39margmax(axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m))\n\u001B[1;32m     41\u001B[0m class_weights \u001B[38;5;241m=\u001B[39m {i: class_weights[i] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(class_weights))}\n\u001B[0;32m---> 43\u001B[0m early_stopping \u001B[38;5;241m=\u001B[39m \u001B[43mEarlyStopping\u001B[49m(monitor\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval_loss\u001B[39m\u001B[38;5;124m'\u001B[39m, patience\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m, restore_best_weights\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     44\u001B[0m model\u001B[38;5;241m.\u001B[39mfit(dataset, epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m20\u001B[39m, batch_size\u001B[38;5;241m=\u001B[39mBATCH_SIZE, steps_per_epoch\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(images_train) \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m BATCH_SIZE, validation_data\u001B[38;5;241m=\u001B[39mval_dataset, validation_steps\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(images_val)\u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39mBATCH_SIZE, callbacks\u001B[38;5;241m=\u001B[39m[early_stopping], class_weight\u001B[38;5;241m=\u001B[39mclass_weights, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m     46\u001B[0m evals \u001B[38;5;241m=\u001B[39m evaluate(model, images_test, labels_test, \u001B[38;5;28mlen\u001B[39m(unique_labels))\n",
      "Cell \u001B[0;32mIn[4], line 43\u001B[0m\n\u001B[1;32m     40\u001B[0m class_weights \u001B[38;5;241m=\u001B[39m sklearn\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mclass_weight\u001B[38;5;241m.\u001B[39mcompute_class_weight(class_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbalanced\u001B[39m\u001B[38;5;124m'\u001B[39m, classes\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39munique(labels_train\u001B[38;5;241m.\u001B[39margmax(axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)), y\u001B[38;5;241m=\u001B[39mlabels_train\u001B[38;5;241m.\u001B[39margmax(axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m))\n\u001B[1;32m     41\u001B[0m class_weights \u001B[38;5;241m=\u001B[39m {i: class_weights[i] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(class_weights))}\n\u001B[0;32m---> 43\u001B[0m early_stopping \u001B[38;5;241m=\u001B[39m \u001B[43mEarlyStopping\u001B[49m(monitor\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval_loss\u001B[39m\u001B[38;5;124m'\u001B[39m, patience\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m, restore_best_weights\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     44\u001B[0m model\u001B[38;5;241m.\u001B[39mfit(dataset, epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m20\u001B[39m, batch_size\u001B[38;5;241m=\u001B[39mBATCH_SIZE, steps_per_epoch\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(images_train) \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m BATCH_SIZE, validation_data\u001B[38;5;241m=\u001B[39mval_dataset, validation_steps\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(images_val)\u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39mBATCH_SIZE, callbacks\u001B[38;5;241m=\u001B[39m[early_stopping], class_weight\u001B[38;5;241m=\u001B[39mclass_weights, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m     46\u001B[0m evals \u001B[38;5;241m=\u001B[39m evaluate(model, images_test, labels_test, \u001B[38;5;28mlen\u001B[39m(unique_labels))\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_310_64.pyx:1187\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_310_64.SafeCallWrapper.__call__\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_310_64.pyx:627\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_310_64.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_310_64.pyx:937\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_310_64.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_310_64.pyx:928\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_310_64.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_310_64.pyx:585\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_310_64.PyDBFrame.do_wait_suspend\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py:1184\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[1;32m   1181\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[1;32m   1183\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[0;32m-> 1184\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py:1199\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[1;32m   1196\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[1;32m   1198\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[0;32m-> 1199\u001B[0m         \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1201\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[1;32m   1203\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Amount of Images\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUMBER_OF_IMAGES = 87770\n",
    "NUMBER_OF_IMAGES = 877\n",
    "IMAGE_SIZE = 224\n",
    "\n",
    "images_global, labels_global = prepare_data(df[:NUMBER_OF_IMAGES], IMAGE_SIZE)\n",
    "\n",
    "images_global = np.repeat(images_global[..., np.newaxis], 3, axis=-1).reshape(-1, IMAGE_SIZE, IMAGE_SIZE, 3)\n",
    "\n",
    "evaluations = {}\n",
    "\n",
    "def normalize_image(img, label):\n",
    "    img = tf.cast(img, np.float32) / 255.0\n",
    "    return img, label\n",
    "\n",
    "\n",
    "for ev_number_of_images in [3000, 13000, 23000, 33000, 43000, 53000, 63000, 73000]:\n",
    "    \n",
    "    ev_number_of_images = ev_number_of_images // 100\n",
    "    \n",
    "    images, labels = images_global[:ev_number_of_images], labels_global[:ev_number_of_images]\n",
    "    images_test, labels_test = images_global[-(int(ev_number_of_images / 5)):], labels_global[-(int(ev_number_of_images / 5)):]\n",
    "\n",
    "    images_train, images_val, labels_train, labels_val = sklearn.model_selection.train_test_split(images, labels, random_state=42, test_size=0.2)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images_train, labels_train))\n",
    "    dataset = dataset.map(normalize_image)\n",
    "    dataset = dataset.shuffle(buffer_size=100)\n",
    "    dataset = dataset.batch(BATCH_SIZE, drop_remainder=False)\n",
    "    dataset = dataset.repeat()\n",
    "    \n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((images_val, labels_val))\n",
    "    val_dataset = val_dataset.map(normalize_image)\n",
    "    val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=False)\n",
    "        \n",
    "    model = create_resnet_model(IMAGE_SIZE)\n",
    "        \n",
    "    class_weights = sklearn.utils.class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(labels_train.argmax(axis=1)), y=labels_train.argmax(axis=1))\n",
    "    class_weights = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    model.fit(dataset, epochs=20, batch_size=BATCH_SIZE, steps_per_epoch=len(images_train) // BATCH_SIZE, validation_data=val_dataset, validation_steps=len(images_val)//BATCH_SIZE, callbacks=[early_stopping], class_weight=class_weights, verbose=2)\n",
    "    \n",
    "    evals = evaluate(model, images_test, labels_test, len(unique_labels))\n",
    "    \n",
    "    evaluations[ev_image_size] = evals\n",
    "    print()\n",
    "    print(evals)\n",
    "    print()\n",
    "    \n",
    "print(evaluations)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-27T14:59:32.331085Z",
     "start_time": "2025-02-27T14:56:50.189197Z"
    }
   },
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (8,) and (0,)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[32], line 21\u001B[0m\n\u001B[1;32m     16\u001B[0m font_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m16\u001B[39m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;66;03m# plt.plot(x_values, accs, label=\"Accuracy\")\u001B[39;00m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# plt.plot(x_values, baccs, label=\"Balanced Accuracy\")\u001B[39;00m\n\u001B[0;32m---> 21\u001B[0m \u001B[43mplt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mplot\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_values\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maucs_at\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mAtelectasis\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     22\u001B[0m plt\u001B[38;5;241m.\u001B[39mplot(x_values, aucs_ef, label\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEffusion\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     23\u001B[0m plt\u001B[38;5;241m.\u001B[39mplot(x_values, aucs_in, label\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInfiltration\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/medicaldeeplearning-6tfcUi4t-py3.10/lib/python3.10/site-packages/matplotlib/pyplot.py:3829\u001B[0m, in \u001B[0;36mplot\u001B[0;34m(scalex, scaley, data, *args, **kwargs)\u001B[0m\n\u001B[1;32m   3821\u001B[0m \u001B[38;5;129m@_copy_docstring_and_deprecators\u001B[39m(Axes\u001B[38;5;241m.\u001B[39mplot)\n\u001B[1;32m   3822\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mplot\u001B[39m(\n\u001B[1;32m   3823\u001B[0m     \u001B[38;5;241m*\u001B[39margs: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m|\u001B[39m ArrayLike \u001B[38;5;241m|\u001B[39m \u001B[38;5;28mstr\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3827\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   3828\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlist\u001B[39m[Line2D]:\n\u001B[0;32m-> 3829\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mgca\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mplot\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3830\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3831\u001B[0m \u001B[43m        \u001B[49m\u001B[43mscalex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mscalex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3832\u001B[0m \u001B[43m        \u001B[49m\u001B[43mscaley\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mscaley\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3833\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdata\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m}\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3834\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3835\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/medicaldeeplearning-6tfcUi4t-py3.10/lib/python3.10/site-packages/matplotlib/axes/_axes.py:1777\u001B[0m, in \u001B[0;36mAxes.plot\u001B[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1534\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1535\u001B[0m \u001B[38;5;124;03mPlot y versus x as lines and/or markers.\u001B[39;00m\n\u001B[1;32m   1536\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1774\u001B[0m \u001B[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001B[39;00m\n\u001B[1;32m   1775\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1776\u001B[0m kwargs \u001B[38;5;241m=\u001B[39m cbook\u001B[38;5;241m.\u001B[39mnormalize_kwargs(kwargs, mlines\u001B[38;5;241m.\u001B[39mLine2D)\n\u001B[0;32m-> 1777\u001B[0m lines \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_lines(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, data\u001B[38;5;241m=\u001B[39mdata, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)]\n\u001B[1;32m   1778\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m lines:\n\u001B[1;32m   1779\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madd_line(line)\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/medicaldeeplearning-6tfcUi4t-py3.10/lib/python3.10/site-packages/matplotlib/axes/_base.py:297\u001B[0m, in \u001B[0;36m_process_plot_var_args.__call__\u001B[0;34m(self, axes, data, return_kwargs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    295\u001B[0m     this \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m args[\u001B[38;5;241m0\u001B[39m],\n\u001B[1;32m    296\u001B[0m     args \u001B[38;5;241m=\u001B[39m args[\u001B[38;5;241m1\u001B[39m:]\n\u001B[0;32m--> 297\u001B[0m \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_plot_args\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    298\u001B[0m \u001B[43m    \u001B[49m\u001B[43maxes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mthis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mambiguous_fmt_datakey\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mambiguous_fmt_datakey\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    299\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_kwargs\u001B[49m\n\u001B[1;32m    300\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/medicaldeeplearning-6tfcUi4t-py3.10/lib/python3.10/site-packages/matplotlib/axes/_base.py:494\u001B[0m, in \u001B[0;36m_process_plot_var_args._plot_args\u001B[0;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001B[0m\n\u001B[1;32m    491\u001B[0m     axes\u001B[38;5;241m.\u001B[39myaxis\u001B[38;5;241m.\u001B[39mupdate_units(y)\n\u001B[1;32m    493\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m!=\u001B[39m y\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]:\n\u001B[0;32m--> 494\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mx and y must have same first dimension, but \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    495\u001B[0m                      \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhave shapes \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mx\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m and \u001B[39m\u001B[38;5;132;01m{\u001B[39;00my\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    496\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m x\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m y\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m2\u001B[39m:\n\u001B[1;32m    497\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mx and y can be no greater than 2D, but have \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    498\u001B[0m                      \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mshapes \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mx\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m and \u001B[39m\u001B[38;5;132;01m{\u001B[39;00my\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mValueError\u001B[0m: x and y must have same first dimension, but have shapes (8,) and (0,)"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 800x600 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "with open(\"pickles/resolution_1000samples_nolr_model3.pkl\", \"wb\") as file:\n",
    "    pickle.dump(evaluations, file)\n",
    "\n",
    "aucs = [evalu[0] for evalu in evaluations.values()]\n",
    "aucs_at = [evalu[\"Atelectasis\"] for evalu in aucs]\n",
    "aucs_ef = [evalu[\"Effusion\"] for evalu in aucs]\n",
    "aucs_in = [evalu[\"Infiltration\"] for evalu in aucs]\n",
    "aucs_nf = [evalu[\"No Finding\"] for evalu in aucs]\n",
    "baccs = [evalu[1] for evalu in evaluations.values()]\n",
    "accs = [evalu[2] for evalu in evaluations.values()]\n",
    "\n",
    "x_values = [50, 100, 150, 200, 250, 300, 350, 400]\n",
    "\n",
    "font_size = 16\n",
    "\n",
    "# plt.plot(x_values, accs, label=\"Accuracy\")\n",
    "# plt.plot(x_values, baccs, label=\"Balanced Accuracy\")\n",
    "\n",
    "plt.plot(x_values, aucs_at, label=\"Atelectasis\")\n",
    "plt.plot(x_values, aucs_ef, label=\"Effusion\")\n",
    "plt.plot(x_values, aucs_in, label=\"Infiltration\")\n",
    "plt.plot(x_values, aucs_nf, label=\"No Finding\")\n",
    "\n",
    "plt.xlabel(\"Resolution in pixels x pixels\", fontsize=font_size)\n",
    "plt.grid(color='grey', linestyle='-', linewidth=0.25, alpha=0.5)\n",
    "plt.legend(fontsize=font_size, loc=\"lower right\")\n",
    "\n",
    "#plt.savefig(\"evals/amount_resnet.pdf\", bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-27T14:51:21.659755Z",
     "start_time": "2025-02-27T14:51:21.409259Z"
    }
   },
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Baseline\n",
    "\n",
    "images, labels = prepare_data(df[:10000], 1)\n",
    "\n",
    "# Calculate class probabilities\n",
    "class_counts = np.sum(labels, axis=0)\n",
    "class_probabilities = class_counts / len(labels)\n",
    "\n",
    "# Prepare the test data\n",
    "images_test, labels_test = prepare_data(df[-2000:], 1)\n",
    "\n",
    "# Generate predictions based on class probabilities\n",
    "predictions = []\n",
    "for _ in range(len(labels_test)):\n",
    "    predicted_class = np.random.choice(len(class_probabilities), p=class_probabilities)\n",
    "    predictions.append(predicted_class)\n",
    "    \n",
    "predictions_proba = np.zeros((len(labels_test), len(class_probabilities)))\n",
    "for i, predicted_class in enumerate(predictions):\n",
    "    predictions_proba[i, predicted_class] = 1\n",
    "    \n",
    "evals = []\n",
    "for class_idx in range(4):\n",
    "    true_labels = labels_test[:, class_idx]  # True labels for this class\n",
    "    pred_probs = predictions_proba[:, class_idx]  # Predicted probabilities for this class\n",
    "\n",
    "    # Calculate the AUC for this class\n",
    "    auc = sklearn.metrics.roc_auc_score(true_labels, pred_probs)\n",
    "    evals.append(auc)\n",
    "    print(f\"AUC for class {class_idx}: {auc}\")\n",
    "    \n",
    "    \n",
    "probabilities_transformed = predictions\n",
    "labels_val_transformed = labels_test.argmax(axis=1)\n",
    "\n",
    "balanced_acc = sklearn.metrics.balanced_accuracy_score(labels_val_transformed, probabilities_transformed)\n",
    "acc = sklearn.metrics.accuracy_score(labels_val_transformed, probabilities_transformed)\n",
    "\n",
    "print(balanced_acc)\n",
    "print(acc)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# tf.keras.utils.plot_model(create_model_2(256), show_shapes=False, rankdir=\"LR\")\n",
    "\n",
    "birne = create_model_3(256)\n",
    "def myprint(s):\n",
    "    with open('test.txt','w') as f:\n",
    "        print(s, file=f)\n",
    "\n",
    "birne.summary(print_fn=myprint)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Train for balanced accuracy\n",
    "class_weights = {}\n",
    "num_classes = len(unique_labels)\n",
    "\n",
    "for class_idx in range(num_classes):\n",
    "    class_count = np.sum(labels[:, class_idx])\n",
    "    class_weight = len(labels) / (num_classes * (class_count + 1e-5)) \n",
    "    class_weights[class_idx] = class_weight\n",
    "\n",
    "\n",
    "model.fit(dataset, epochs=10, batch_size=BATCH_SIZE, steps_per_epoch=len(images) // BATCH_SIZE, class_weight=class_weights, validation_data=(images_val, labels_val))\n",
    "probabilities = model.predict(images_test)\n",
    "predictions = probabilities.argmax(axis=1)\n",
    "labels_test = labels_test.argmax(axis=1)\n",
    "\n",
    "balanced_acc = sklearn.metrics.balanced_accuracy_score(labels_test, predictions)\n",
    "\n",
    "balanced_acc"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medicaldeeplearning-6tfcUi4t-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
