{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "610e63bf",
   "metadata": {},
   "source": [
    "# Medical Image Classification with CNNs\n",
    "This notebook demonstrates importing, preprocessing, and training CNN models on the NIH Chest X-rays dataset using transfer learning. Models like VGG16, ResNet50, and EfficientNet are utilized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1ac498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16, ResNet50, EfficientNetB0\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63be264",
   "metadata": {},
   "source": [
    "## Load and Preprocess the Dataset\n",
    "Assuming the NIH Chest X-rays dataset is downloaded and stored locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b2abed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dataset paths (adjust paths as needed)\n",
    "data_dir = '/path/to/chest_xray_dataset'  # Update to your dataset path\n",
    "train_dir = os.path.join(data_dir, 'train')\n",
    "test_dir = os.path.join(data_dir, 'test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47b001b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation and Preprocessing\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'  # Adjust as needed\n",
    ")\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e208b2",
   "metadata": {},
   "source": [
    "## Define CNN Model Using Transfer Learning\n",
    "We will use VGG16, ResNet50, and EfficientNetB0 architectures for transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dacd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model creation function\n",
    "def build_model(base_model):\n",
    "    base_model.trainable = False\n",
    "    x = base_model.output\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    predictions = Dense(1, activation='sigmoid')(x)\n",
    "    return Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Initialize models\n",
    "vgg16_base = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "resnet50_base = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "efficientnet_base = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "models = {\n",
    "    'VGG16': build_model(vgg16_base),\n",
    "    'ResNet50': build_model(resnet50_base),\n",
    "    'EfficientNetB0': build_model(efficientnet_base)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8f5920",
   "metadata": {},
   "source": [
    "## Train and Evaluate the Models\n",
    "Compile, train, and evaluate each model on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba14eb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and train models\n",
    "for name, model in models.items():\n",
    "    print(f'Training {name}...')\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        epochs=10,\n",
    "        validation_data=test_generator,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluate model\n",
    "    print(f'Evaluating {name}...')\n",
    "    test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "    print(f'{name} Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')\n",
    "\n",
    "    # Generate classification report and confusion matrix\n",
    "    y_true = test_generator.classes\n",
    "    y_pred = np.round(model.predict(test_generator)).astype(int)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d')\n",
    "    plt.title(f'{name} Confusion Matrix')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
